\documentclass[12pt]{article}
\usepackage{cmap} % тоже для кодировки
%\usepackage[cp866]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc} % любая желаемая кодировка
\usepackage[english]{babel}
\usepackage[pdftex,unicode]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\usepackage{extsizes}
\usepackage{float}
\usepackage{bbold}
\usepackage{dsfont}
\usepackage{calc}
\usepackage{bm}
%\usepackage [a4paper,% other options: a3paper, a5paper, etc
%  left=3cm,
%  right=1.5cm,
%  top=2cm,
%  bottom=2cm,
%]{geometry}

\usepackage{tocloft}
%\renewcommand{\cfttoctitlefont}{\hspace{0.38\textwidth} \bfseries}
%\renewcommand{\cftbeforetoctitleskip}{-1em}
%\renewcommand{\cftaftertoctitle}{\mbox{}\hfill \\ \mbox{}\hfill{\footnotesize Стр.}\vspace{-.5em}}
%\providecommand{\cftchapfont}{\normalsize\bfseries \sectionname}
%\renewcommand{\cftsecfont}{\hspace{31pt}}
%\renewcommand{\cftsubsecfont}{\hspace{11pt}}
%\providecommand{\cftbeforechapskip}{1em}
%\renewcommand{\cftparskip}{-1mm}
%\renewcommand{\cftdotsep}{1}
%\setcounter{tocdepth}{2} % задать глубину оглавления - до subsection включительно

\usepackage{titlesec}
\sloppy
%\titleformat{\section}
%{\normalsize\bfseries}
%{\thesection}
%{1em}{}

%\titleformat{\subsection}
%{\normalsize\bfseries}
%{\thesubsection}
%{1em}{}

% Настройка вертикальных и горизонтальных отступов
%\titlespacing*{\chapter}{0pt}{-30pt}{8pt}
%\titlespacing*{\section}{\parindent}{*4}{*4}
%
%\linespread{1.3}

\newlength{\widecommentlength}
\setlength{\widecommentlength}{5in}
% \newcommand{\widecommentbox}[2]{\def#1##1{\strut\newline\noindent\colorbox{#2}{\linespread{1}\parbox{.95\textwidth}{\small ##1}}\newline}}
% \usepackage{pgfplots}
\newcommand{\widecommentbox}[3]{\def#1##1{\strut\newline\noindent\colorbox{#3}{\linespread{1}\parbox{.95\textwidth}{\small {\bf [#2]} ##1}}\newline}}
\def\commentsep{\noindent\dotfill}
\newcommand\inner[2]{\langle #1, #2 \rangle}

% To temporarily omit all comments, enable these two lines:
% \renewcommand{\widecommentbox}[3]{\def#1##1{}}
% \let\commentsep\relax

\widecommentbox{\alex}{Alex}{green!20!white}
\widecommentbox{\ad}{AD}{red!20!white}

\usepackage{enumitem}
\usepackage{setspace}

\makeatletter
%\@addtoreset{theorem}{section}
%\@addtoreset{lemma}{section}
\@addtoreset{prop}{section}
\makeatother

%\newcommand{\sectionbreak}{\clearpage}
\usepackage[square,numbers,sort&compress]{natbib}
\usepackage{mathtools}
\renewcommand{\bibnumfmt}[1]{#1.\hfill} % нумерация источников в самом списке - через точку
% \renewcommand{\bibsection}{\section*{Список литературы}} % заголовок специального раздела
\setlength{\bibsep}{0pt}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}%

%\titleformat{\section}[block]{\Large\bfseries\centering}{}{1em}{}
%\titleformat{\subsection}[block]{\large\bfseries\centering}{}{1em}{}
\renewcommand{\cal}[1]{\mathcal{#1}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\phi}{\varphi}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{prop_under_lemma}{Утверждение}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{corol}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem*{note}{Note}
\newtheorem{definition}{Definition}
\newtheorem{example}{Пример}
\newcommand\bigmatrixzero{\raisebox{-0.25\height}{\textnormal{\Huge 0}}}
\newcommand\bigzero{\makebox(10, 10){\text{\Huge 0}}}
\newcommand{\seq}[1]{\{{#1}_n\}_{n=1}^\infty}
\newcommand{\fsys}{\mathrm{F}}
\newcommand{\sgnwt}{\mathrm{W}^{sgn}}
\newcommand{\wt}{\mathrm{W}}
\newcommand{\flow}{\mathcal{F}}
\newcommand{\flowpos}{\mathcal{F^+}}
\newcommand{\flowsgn}{\mathcal{F}_{sgn}}
\newcommand{\source}{\mathit{source}}
\newcommand{\sink}{\mathit{sink}}
\renewcommand{\root}{\mathit{root}}
\newcommand{\scal}[2]{\langle {#1}, {#2} \rangle}
\newcommand{\net}{\Delta}
\newcommand{\onet}{\vec{\Delta}}

\numberwithin{remark}{section}
\numberwithin{theorem}{section}
\numberwithin{prop}{section}
\numberwithin{equation}{section}
\numberwithin{lemma}{section}
\numberwithin{prop_under_lemma}{lemma}

\begin{document}
% \title{Методы суммирования ряда Фурье \\
% относительно системы Азоффа--Шехада}

% \author{Алексей Пышкин\thanks{Работа поддержана грантом Президента РФ для государственной 
% поддержки молодых российских учёных -- докторов наук МД-5758.2015.1.}}
% \date{}

\section{B-class of vector system examples}
    We are interested in the completeness properties of the minimal vector system $\fsys = \{f_n\}$ in a separable real [TODO complex] Hilbert space $\cal{H}$.
    Suppose that $\cal{H}$ has an orthonormal basis $\{e_j\}_{j=1}^\infty$.
    Let $\{f^*_n\}$ be a biorthogonal vector system to the original system $\{f_n\}$.
    \begin{definition}
        The pair of vector systems $\{f_n\}, \{f^*_n\}$ belongs to the \textit{B-class} whenever the following conditions are satisfied:
        \begin{itemize}
            \item either $f_n = e_n$ or $f^*_n = e_n$ for any $n > 0$,
            \item $\langle f_n, e_n\rangle = \langle f^*_n, e_n \rangle = 1$ for any $n > 0$,
            \item $\langle f_n, e_k \rangle = -\langle f^*_k, e_n \rangle$ for any $n, k > 0$,
            \item the matrices $\{\langle f_n, e_k\rangle\}$ and $\inner{f^*_n}{e_k}$ are both finite-band.
        \end{itemize}
    \end{definition}
    This class of vector systems is the subject of this text.
    \begin{prop}
        The definition given above guarantess the biorthogonality of the $f_n$ and $f^*_n$.
    \end{prop}
    \begin{proof}
        Straightforward check.
    \end{proof}
    \begin{remark}
        Note that the Azoff-Shehada example~\cite{azoff} lies in the B-class.
    \end{remark}
    
    B-class has an interesting property we are going to exploit later.
    \begin{prop}
        A vector system which belongs to the B-class could be associated with a 
        weighted bipartite graph $B(\{f_k\}) = (V, E, \sgnwt)$, where $\sgnwt$ is real-valued function on the $V\times V$ such that
        $\sgnwt(v, u) = -\sgnwt(u, v)$.
        Also the inverse is true: for each bipartite graph $(V, E, \sgnwt)$ for which the weight function
        $\sgnwt$ satisifies the conditions above there is a vector system from the B-class.
    \end{prop}
    \begin{proof}
        For each index $l > 0$ such that $f^*_l = e_l$ we put a vertex $v_l$ in the first part of the bipartite graph.
        We will call this part from now on the \textit{left} part of the graph $B(\fsys)$.
        For any other index $r > 0$ we construct a vertex in the other part of the graph.
        Evidently, for such indices $r > 0$ the condition $f_r = e_r$ holds due to the definition of the B-class vector systems.
        The second part of the graph will be referred as the \textit{right} part of the graph $B(\fsys)$.
        We put an edge between two vertices $v_l$ and $v_r$ from the left and right parts respectively,
        whenever the scalar product $\langle f_l, e_r \rangle$ is not zero.
        For such two vertices we have $\langle f_l, e_r \rangle$ = $-\langle f_r, e_l \rangle$.
        Denote by $\sgnwt_{lr}$ the expression $\sgnwt_{lr} = \langle f_l, e_r \rangle^{-1}$.
        Obviously $\sgnwt_{lr} = -\sgnwt_{rl}$.
        //todo inverse
    \end{proof}
    We are going to find the necessary and sufficient condition for the $k$-completeness property for the B-class vector system.
    In order to pursue that we need to investigate the conditions, under which the B-class system admits a linear summation method.
    First of all we intend to demonstrate an elegant reformulation of the linear summation method existence problem for
    the B-class vector systems.
    Recall that there exists a linear summation method for the system $\{f_n\}, \{f^*_n\}$
    iff there is no operator $T$ with the trace equal to $1$, such that $\langle Tf_n, f_n^*\rangle = 0$ for any $n$.
    Suppose that there is a bounded operator $T$ such that $\langle Tf_n, f_n^*\rangle = 0$ for any $n$.
    There are two cases: either $f^*_n = e_n$ or $f_n = e_n$.
    In the first case the condition $\langle Tf_n, f_n^*\rangle = 0$ turns into
    \begin{equation}
        \label{left-eqn}
        \sum_j T_{nj} \langle f_n, e_j \rangle = 0,
    \end{equation}
    and in the second case it is equivalent to 
    \begin{equation}
        \label{right-eqn}
        \sum_j T_{jn} \langle f^*_n, e_j \rangle = 0.
    \end{equation}
    Now consider an auxilary function $\flowsgn(T): E \to \mathbb{R}$ 
    on the edges of the graph $B(\fsys)$.
    \begin{equation*}
        \flowsgn(v_l, v_r) := T_{lr} \langle f_l, e_r \rangle,\\
        \flowsgn(v_r, v_l) := T_{lr} \langle f^*_r, e_l \rangle.
    \end{equation*}
    Observe that $\flowsgn(T)$ is a skew-symmetric function.
    Moreover, the equalities~\eqref{left-eqn} and~\eqref{right-eqn} are reduced to the simple
    \begin{equation}
      \label{almost-flow-eqn}
      \sum_{u \in V} \flowsgn(v, u) + T_{vv} = 0
    \end{equation}
    for each vertex $v$ from the graph $B(\fsys)$.
    \begin{remark}
      Notice how the function $\flow$ defined on the graph $B(\fsys)$ resembles
      a \emph{flow} function defined on the edges of the $B(\fsys)$ graph.
      One might also see that the equation~\eqref{almost-flow-eqn} describes the total flow (sum of the outgoing flow and incoming flow) 
      for each vertex $v$ in the graph $B(\fsys)$
      In order to formalize this observation we are going to build a flow function
      on a slightly modified graph $BG(\fsys)$.
    \end{remark}
    Befory we proceed we are going to introduce a few basic definitions.
    \begin{definition}
        \textit{Network} $\net$ is a quadruple $(G, \wt, s, t)$, where $G = (V, E)$ is weighted graph 
        with a positive weight function $\wt$ on $E$ and two vertices $s, t \in V$, which
        we will call $\source$ and $\sink$ of the network respectively.
    \end{definition}
    \begin{definition}
        Look at the skew-symmetric function $\flow: V \times V \to R$.
        The function $\flow$ will be called a \textit{pseudo-flow}.
    \end{definition}
    \begin{definition}
      Let $\net = (G, \wt, s, t)$ be a network.
      For each vertex $v \in V$ we take $IN(v)$ as the set of incoming edges
      and $OUT(v)$ as the set of outgoing edges in the graph $G$.
      Also denote by $d_{+}(v)$ the sum of the flows \textit{leaving} the node $v$ and by
      $d_{-}(v)$ the sum of the flows \textit{entering} $v$:
      \begin{align*}
          d_{+}(v) = \sum_{u \in OUT(v)} \flow(vu),\\
          d_{-}(v) = \sum_{u \in IN(v)} \flow(uv).
      \end{align*}
      Also let $d(v)$ be equal to $d_{+}(v) - d_{-}(v)$. Sometimes we will refer to this value as
      a \textit{total flow} of the vertex $v$.
      The vertex is called $deficient$ iff $d(v)$ is less than zero,
      $active$ iff $d(v)$ is greater than zero and
      $preserving$ iff $d(v)$ is precisely zero, meaning that the total incoming flow
      is equal to the total outcoming flow of the vertex $v$.
    \end{definition}
    \begin{definition}
      Given a network $\net = (V, E, \wt, s, t)$ and a pseudo-flow function $\flow$ we will name $\flow$ a flow function iff 
      any vertex $v \in V \setminus \{s,t\}$ \textit{preserves} the flow, namely $d(v) = 0$.
    \end{definition}
    \begin{definition}
      For a \textit{network} $\net = (G, \wt, s, t)$ and a flow $\flow$ we will say,
      that the flow $\flow$ is \textit{preserving} if $d(\mathit{sink}) = -d(\mathit{source})$ vertex.
    \end{definition}
    \begin{note}
      In simple words this network property suggests, 
      that the total flow coming out of the \textit{source} is equal to the total flow
      coming into the \textit{sink}.
    \end{note}
    \begin{definition}
        \textit{Oriented network} $\onet$ is a quadruple $(\vec{G}, \wt, s, t)$, where
        $\vec{G} = (V, \vec{E})$ is weighted oriented graph 
        with a positive weight function $\wt$ on $\vec{E}$ and two vertices $s, t \in V$, which
        we will call $\source$ and $\sink$ of the network respectively.
    \end{definition}
    Suppose $\net = (V, E, \wt, s, t)$ is a network and $\flow$~--- is a skew-symmetric function defined
    on $\net$.
    Naturally, we might instead consider an oriented network $\onet = (V, \vec{E}, \wt, s, t)$, where
    the set of vertices is the same.
    We set the direction of the edge $(uv)$ is as follows:
    \begin{itemize}
      \item $(uv) \in \vec{E}$ if $\flow(uv) > 0$,
      \item $(vu) \in \vec{E}$ if $\flow(uv) < 0$.
    \end{itemize}
    This way only one of the edges $(uv)$ and $(vu)$ stays in the $\vec{G} = (V, \vec{E})$.
    Also we set up the positive flow function $\flowpos: E \to \mathbb{R}^+$ such that 
    for any edge $\vec{e} = (uv) \in \vec{G}$
    $$
      \flowpos(uv) = |\flow(uv)|.
    $$
    \begin{definition}
      The \emph{oriented} network $\onet$ we will call an orientation of the network $\net$.
    \end{definition}
    \begin{definition}
      Consider a network $\net = (G, \wt, s, t)$ such that
      the degree of each vertex in $V \setminus \{s, t\} $ is finite,
      and the weight of each edge incident to $\source$ or to $\sink$ is equal to one.
      We will call such network a \it{B-class network}.
    \end{definition}
    
    Now we are ready to continue with the set up.
    Our aim is to set up a network from the graph $BG(\fsys)$.
    We are to construct the \emph{oriented} graph $BG(\fsys) = (V, E)$ from the graph $B(\fsys)$,
    positive weight function $\wt: E \to R^{+}$ and a positive flow function
    $\flow: E \to R^{+}$.
    Firstly we consider the vertices and the edges of the graph $B(\fsys)$.
    Let us start from all the vertices in the graph $B(\fsys)$,
    all of which we incorporate into the graph $BG(\fsys)$.
    Since the vertex set of the new graph is a superset of vertices of the graph $B(\fsys)$,
    we will continue using the partition we made earlier among the vertices of the graph $B(\fsys)$, and
    refer to the vertices as the vertices from the \textit{left} part of the graph and the vertices from the 
    \textit{right} part of the graph.
    The graph $BG(\fsys)$ will contain all the edges from the graph $B(\fsys)$ as well.
    For each of these edges we are going to choose an orientation, assign a flow function $\flow$ and
    establish a new positive weight function $\wt$.
    Each edge $e = (v_l v_r)$ from the graph $B(\fsys)$ we include in the graph $BG(\fsys)$ with such a direction:
    if an auxilary function $\flowsgn(v_r, v_l) [= T_{lr} \langle f^*_r, e_l \rangle]$ is greater than zero, then
    we orient the edge from $v_r$ to $v_l$, if not -- then we choose the other orientation.
    Now we fix the flow function and the weight function on the edge $e = (v_l v_r)$:
    \begin{align*}
      \flow(e) = \left| \flowsgn(v_r, v_l) \right|,\\
      \wt(e) = \left| \sgnwt(e)\right|.
    \end{align*}
    Due to the skew-symmetry of the original weight function $\sgnwt$ and auxilary function $\flowsgn$
    these definitions are correct.
    
    Now we put two additional vertices to the already constructed graph:
    the vertex $\source$ and the vertex $\sink$.
    Suppose we have enumerated the vertices from the left part of the graph, for instance $\{l_k\}_{k=1}^\infty$.
    Then for each $k > 0$ we connect the $\source$ vertex and the $l_k$ vertex with
    the edge $e_k = (\source l_k)$, which is oriented in such a way that the $\flowsgn$ function
    is greater than zero. Namely, for such edges $e_k$ we set:
    \begin{align*}
      \flow(l_k \source) = T_{l_k l_k}, \quad \text{ if } T_{l_k}{l_k} > 0,\\
      \flow(\source l_k) = -T_{l_k l_k}, \quad \text { if } T_{l_k}{l_k} < 0,
    \end{align*}
    In the first case we orient the edge from the $l_k$ vertex to $\source$ and in the second case from the 
    $\source$ vertex to $l_k$.
    We assign the weight equal to one for such edges: $\wt(e) = 1$.
    As we added the edges ${e_k}$, the flow became \emph{preserved} at each vertex of the left part of the graph $BG(\fsys)$.
    In the similar manner we will transform the right part of the graph.
    Assuming we have the vertices from the right part of the graph $B(\fsys)$ in the form of the sequence $\seq{r}$,
    for each $k > 0$ we connect the $r_k$ vertex to the $\sink$ with the edge $e'_k$
    and set the flow equal to:
    \begin{align*}
      \flow(r_k \sink) = T_{r_k r_k}, \quad \text{ if } T_{r_k r_k} > 0,\\
      \flow(\sink r_k) = -T_{r_k r_k}, \quad \text { if } T_{r_k r_k} < 0,
    \end{align*}
    For these edges we put weight equal to one as well.
    The graph $BG(\fsys)$ is built up, and the pair $\net(\fsys, T) = (BG(\fsys), \flow)$ is a network.
    Indeed, $\net(\fsys, T)$ is a network since each of the vertex preserves
    the flow due to the equation~\eqref{almost-flow-eqn}.
    \begin{remark}
      Sometimes it might be useful to consider slightly different definition of the network.
      Instead of studying an oriented graph with a positive flow function on it we might consider
      an undirected graph with a skew-symmetric flow function on the edges.
      Intuitively, the direction of the flow could be expressed either via the direction of the edge
      or with the sign of the flow function, and these two definitions describe the same objects.
      Formally there is a isomophism between these two types of objects.
    \end{remark}
    \begin{definition}
      Let $\net=(V,E,\flow)$ be a network -- an oriented graph with a positive flow function $\flow$ on it.
      Consider an undirected graph $G=(V,E')$ such that $e=(vu)$ belongs to $E'$ iff
      either $(vu) \in E$ or $(uv) \in E$.
      Such undirected graph we will call the \it{frame} of the network $\net$.
      Also define a function $\flowsgn$ on the $V \times V$:
      \begin{align*}
        \flowsgn(vu) = \flow(vu), \quad \text{if edge } (vu) \in E,\\
        \flowsgn(vu) = -\flow(vu), \quad \text{if edge } (uv) \in E.
      \end{align*}
      Such function we will call \it{sign-flow} function.
    \end{definition}
    \begin{remark}
      Notice that the frame for the contstructed network $\net(\fsys, T)$ is the weighted graph $B(\fsys)$
      we considered in the first place.
      Hence the built network might be considered as the weighted graph $B(\fsys)$ with a sign-flow function
      $\flowsgn$ defined on it.
    \end{remark}
    \begin{remark}
      Pay you attention to the fact that the frame of the network $\net(\fsys, T)$, which is precisely $B(\fsys)$,
      depends only on the biorthogonal system $\fsys$ and not on the operator $T$.
      Only the sign-flow function depends on the operator $T$.
      This way me are able to separate the graph structure and the flow defined on it.
    \end{remark}
    What can we say about the total flow in each of the vertex in the constructed network $\net(\fsys, T)$?
    Due to the the trick above, the total flow became zero in each of the vertices from the left and the right parts.
    The total flow $d(\source)$ in the $\source$ vertex is equal to the $\sum T_{l_k l_k}$, and the total flow in the
    $\sink$ vertex is now equal to the $\sum T_{r_k r_k}$.
    Now one can see that the built network preserves the flow if and only if the trace of the operator $T$ is equal to
    zero.
    \begin{definition}
      Consider a weighted network $\net = (V, E, \flow, \wt)$.
      We introduce a positive function $\|\flow\|$, which we call the \it{mass of the flow} $\flow$.
      $$
        \|\flow\| = \sum_{e \in E} \flow(e) \wt(e).
      $$
      We allow it to be equal $\infty$ as well.
    \end{definition}
    \begin{theorem}
      \label{thm-graph-eq}
        A B-class system $\seq{f}$ admits a linear summation method if and only if for any 
        B-class network $\net = (BG(\fsys), \flow, \wt)$, such that the flow $\flow$ is of the finite mass:
        $$
          \|\flow\| = \sum_{e \in E} \flow(e) \wt(e) < \infty,
        $$
        preserves the flow.
    \end{theorem}
    \begin{proof}
      First we shortly repeat the sufficiency proof we sketched before.
      Suppose there is no summation method for the system $\seq{f}$.
      We already mentioned, that it implies, that there exists a trace class operator $T$ with
      the trace equal to one, such that $\scal{Tf_n}{f_n^*} = 0$ for any $n$.
      Using the operator $T$, we were able to built up the weighted network
      $\net(\fsys, T, \wt)$ which does NOT preserve the flow. 
      It is a B-class network since all the edges incident to $\source$ or $\sink$ have the weight
      equal to one.
      The only thing left to check is that the flow we constructed has a finite mass.
      However it easy follows from the the fact that the operator matrix is finite band and has a finite trace.
      Thus we arrived to contradiction and the sufficiency is proved.
      Now suppose there is a B-class system $\seq{f}$ and it admits a linear summation method.
      Assume there is also a B-class network $\net = (BG(\fsys), \flow, \wt)$ which does not preserve the flow.
      Now we are going to construct the operator $T$ with non-zero trace, which annihilates all the rank-one
      operators $f_n \otimes f^*_n$, thus coming to the contradiction with our initial assumption.
      First consider the edges to the $\source$ vertex. Having all the vertices already enumerated
      we will consider the vertices in the left part $v_{l_k}$.
      Recall that each vertex $v_s$ (except $\source$ and $\sink$) matches to the basis element $e_s$.
      Assign the diagonal elements of $T$:
      $$
        T_{l_k l_k} := \flow(l_k, \source) \wt(l_k l_k) = \flow(l_k, \source) \quad k > 0.
      $$
      Now we repeat the same procedure for the right part of the graph $BG(\fsys)$, thus having
      $$
        T_{r_k r_k} := \flow(r_k, \sink) \wt(r_k r_k) = \flow(r_k, \sink) \quad k > 0.
      $$
      Now consider two connected vertices $v_l$, $v_r$ from the left and right part
      respectively. Assign
      $$
        T_{lr} := \flowsgn(v_l, v_r) * \wt(v_l, v_r).
      $$
      Let all the other matrix elements of the operator matrix $T_{ij}$ be zero.
      Then observe that we got a finite-band operator matrix $T_{ij}$ with the sum of diagonal elements
      equal to one.
      Due to the finite mass of the flow $\flow$ the $T_{ij}$ is an absolute summable sequence.
      Then the operator $T$ we obtained is compact and acquires a finite non-zero trace.
      Due to the fact, that the flow was preserved at each vertex of the network $\net(\fsys, \flow, \wt)$,
      we see that the operator $T$ annihilates all the rank one operators $f_n \otimes f^*_n$,
      which leads to a contradiction.
    \end{proof}
    
    \subsection{B-class networks characterisation}
    Due to the theorem~\ref{label-graph-eq} we intend to analyze the B-class networks
    $BG(\fsys), \flow, \wt)$ in order to understand the conditions under which a linear
    summation method exists for $\fsys$.
    In this section we are not going to address the initial Hilbert space setup at all,
    we will use only the reformulation we obtained in the last theorem.
    Remember that
    \begin{definition}
      Let $G = (V, E)$ be an infinite (oriented) graph.
      An infinite path $r = \seq(v)$ is called a \it{ray} iff
      $r$ is a path, meaning there is an edge $e = (v_k v_{k+1})$ for each $k > 0$ in $G$.
    \end{definition}
    The main result of this section is a following
    \begin{theorem}
      Let $BG = (V, E)$ be a B-class network frame with a skew-symmetric weight function $\sgnwt : V \times V \to \mathbb{R}$.
      Then there exist such signed flow $\flowsgn$ that the network $\net = (BG, \flowsgn, \wt)$ has the following properties:
      \begin{itemize}
        \item the flow $\flowsgn$ has a finite mass,
        \item the network $\net$ does not preserve the flow,
      \end{itemize}
      if and only if there exists a ray $r = \seq{v}$, starting from the vertex $\source$
        in the network $\net$ such that it has a finite weight:
      $$
        \sum \wt(v_k v_{k+1}) < \infty.
      $$
    \end{theorem}
    \begin{proof}
      In order to make our proof easier we merge the $\source$ vertex with the $\sink$ vertex and call this a
      new $\source$ vertex.
      This way we obtain a network with a single $\source$ for which $d(\source) = 1$.
      %We might take any other vertex as a new \sink vertex in order for this new graph b
      All the other vertices in the graph preserve the flow after this procedure.
      Also since all the edges incident to the $\source$ vertex have a weight equal to one
      (see the definition of the B-class networks) the existence of the ray with a finite total weight
      in the transformed network is equivalent to the existence of such ray in the original network.
      First we prove the sufficiency.
      Suppose that there is a ray of a finite total weight. 
      In that case we set the resulting flow function $\flow(v_k v_{k+1})$ to one, creating an elusive flow
      from the $\source$ to infinity. All the other edges will not carry any flow, hence the flow is preserved
      at each vertex but not preserved by the network $\net$.
      Now let us turn to the more difficult part, where we prove the necessity.
      In this part we are going to view this network as a rooted graph with the root at the $\source$ vertex.

      Firstly, let us get rid of all the flow circuits in the oriented graph $G$.

      Namely, we will reason by induction and at each step $n$ construct a subnetwork
      $\net_n = (G_n, \flow_n, \wt)$, such that $\flow_n(e) \leq \flow(e)$ for any $e \in E_n$ for any $n$
      and $\flow_{n+1}(e) < \flow_n(e)$.
      consider the subnetwork $G_n = (V_n, E_n)$, where the vertices
      $V_n = {v_k}_{k=1}^n$ and the edges $E_n$ is the edges incident to the $V_n$.
      Let there be a circuit $C_1$ in the graph $G_n$.
      At this point we can decrease the flow on the edges of $C_1$ in such a way, that the total mass
      of the flow will not increase.
      Let $m_1 = \min_{e \in C_1}(\flow(e))$.
      Consider a new flow

    \end{proof}
    \subsection{Application of the B-class networks characterisation}
    %----------------------------------------------------------------------------------------
    \pagebreak
    \begin{prop}
        The given system is an $M$-basis if the coefficients satisfy the restrictions above.
    \end{prop}
    \begin{proof}
        The equality $c_n + d_n = a_n b_n$ guarantees the bi\-orthogonality,
        while the completeness of the $\{f_n\}$ and $\{f_n^*\}$ is
        easy to check.
    \end{proof}
    
    \begin{theorem}
        The given system is NOT $k$-complete for any $k > 1$ iff the sequence
        $$
            \mu_n = \min(1/|a_n| + 1/|b_n|, (1 + |b_n|)/|d_n|, (1 + |a_n|)/|c_n|)
        $$ belongs to $l^1$.
    \end{theorem}
    \begin{proof}
        Let us consider a $k$-dimensional operator $T$ such that 
        $Tr(TR) = 0$ for each $R \in \cal{R}_1(\cal{A})$ which essentially means that
        $\langle Tf_n, f_n^* \rangle = 0$ for any $n$. 
        Notice that the partial sums of the fourier series for the given system are somehow close to the
        partial sums of the canonical fourier series (using the orthonormal basis $e_n$). Define
        $$
          \Xi_n := \sum_1^n \langle Tf_s, f_s^* \rangle - \sum_1^n \langle Te_s, e_s \rangle = -\sum_1^n \langle Te_s, e_s \rangle.
        $$
        where the $\langle \cdot, \cdot\rangle$ denotes a standard scalar product in $\cal{H}$.
        These residuals has also a concise form:
        \begin{align*}
            \Xi_{4j} = a_{2j} T_{4j+1, 4j} + c_{2j} T_{4j+2, 4j},\\
            \Xi_{4j + 1} = -d_{2j} T_{4j+2, 4j} + b_{2j} T_{4j+2, 4j+1},\\
            \Xi_{4j + 2} = a_{2j+1} T_{4j+2, 4j+3} + c_{2j+1} T_{4j+2, 4j+4},\\
            \Xi_{4j + 3} = -d_{2j+1} T_{4j+2, 4j+4} + b_{2j+1} T_{4j+3, 4j+4},
        \end{align*}
        having $T_{ij}$ equal to the $\langle Te_j, e_i\rangle$.
        \begin{prop}
            \label{inf-dim-statement}
            There exists an operator $T$ with trace equal to $1$ such that
            $\langle Tf_n, f_n^*\rangle = 0$ for any $n$ if and only if the
            sequence from the statement of the theorem $\mu_n$ belong to $l^1$.
        \end{prop}
        \begin{proof}
            Assume $\mu_n$ is a $l^1$ sequence. We will construct a required operator $T$.
            Let $T_{11}$ be equal to $-1$, and $T_{jj}$ be equal to zero for any other $j$.
            Then for each $n$ we have three cases:
            1. Suppose $\mu_n = 1/|a_n| + 1/|b_n|$. Then if $n$ is $2j$ for some integer $j$, then
            let:
            \begin{align*}
                T_{4j+1,4j}&=1/a_n & \quad T_{4j+2,4j} = 0,\\
                T_{4j+2,4j+1}&=1/b_n.
            \end{align*}
            That guarantees an equality $\Xi_{4j} = \Xi_{4j+1} = 1$.
            If $n$ is $2j+1$ then let:
            \begin{align*}
                T_{4j+2,4j+3}&=1/a_n & \quad T_{4j+2,4j+4} = 0,\\
                T_{4j+3,4j+4}&=1/b_n,
            \end{align*}
            which provides an equality $\Xi_{4j+2} = \Xi_{4j+3} = 1$.
            2. Consider the case when $\mu_n$ is equal to $(1 + |b_n|)/|d_n|$. 
            Suppose $n = 2j$ for some natural $j$.
            Here we assign
            \begin{align*}
                T_{4j+1,4j} &= b_{2j}/d_{2j} & \quad T_{4j+2,4j} = -1/d_{2j},\\
                T_{4j+2,4j+1} &= 0.
            \end{align*}
            The case $n = 2j + 1$ is essentially the same.
            3. Consider the case when $\mu_n$ is equal to $(1 + |a_n|)/|c_n|$. 
            Suppose $n = 2j + 1$ for some natural $j$.
            Here we assign
            \begin{align*}
                T_{4j+2,4j+3} &= 0 & \quad T_{4j+2,4j+4} = 1/c_{2j+1},\\
                T_{4j+3,4j+4} &= a_{2j+1}/c_{2j+1}.
            \end{align*}
            The case $n = 2j$ is essentially the same.
            All the other entries $T_{ij}$ we are setting to zero.
            The constructed operator $T$ obviously belongs to the trace class (since all the non-zero elements are summable 
            on the assumption that $\mu_n$ is a $l^1$ sequence).
            These assignments ensure that all the $\Xi_n$ are equal to $1$ for any $n > 1$.
            Since the trace of the operator $T$ is equal to
            $1$ and it annihilates all the rank one operators $f_n \otimes f^*_n$, the sufficiency is proved.
            \medskip\\
            Now assume that there exists a trace class operator $T$ with all the properties we require.
            Then obviously all the $T_{nn}, T_{n, n+1}, T_{n, n+2}$ elements are summable (simple property of
            a trace class operator matrix).
            As we know $\Xi_n$ tends to $1$ since the trace of the operator is $1$. Let us consider a sum
            $|T_{4j+1, 4j}| + |T_{4j+2,4j}| + |T_{4j+2,4j+1}|$. It depends linearly on $T_{4j+2, 4j}$ and the minimum
            is obtained on the boundary of the domain. The minimum value is greater than $\mu_{2j}/2$ whenever $j$ is
            sufficiently large.
            The second pair of equalities gives out the minimum value greater than $\mu_{2j+1}/2$ when $j$ is large,
            which shows that the stated condition is necessary for the existence of operator $T$.
        \end{proof}
        Now consider the case of the $k$-dimensional operator $T$.
        Let us put the operator $T$ as a sum of $k$ rank one operators:
        $$
            T = \sum_1^k y^s \otimes x^s,
        $$
        where $x^s, y^s \in \cal{H}$
        Therefore $T_{ij} = \sum {y^s_j x^s_i}$.
        Let us define vectors $v_n$ and $u_n$ which lie within $\mathbb{R}^k$ as follows:
        \begin{align*}
            v_{2j} &= (y^1_{4j}, y^2_{4j}, \dots ,y^k_{4j}) \quad
            &v^*_{2j} = (x^1_{4j}, x^2_{4j}, \dots ,x^k_{4j}) \\
            v_{2j+1} &= (x^1_{4j+2}, x^2_{4j+2}, \dots ,x^k_{4j+2}) \quad
            &v^*_{2j+1} = (y^1_{4j+2}, y^2_{4j+2}, \dots ,y^k_{4j+2}) \\
            u_{2j} &= (x^1_{4j+1}, x^2_{4j+1}, \dots ,x^k_{4j+1}) \quad
            &u^*_{2j} = (y^1_{4j+1}, y^2_{4j+1}, \dots ,y^k_{4j+1}) \\
            u_{2j+1} &= (y^1_{4j+3}, y^2_{4j+3}, \dots ,y^k_{4j+3}) \quad
            &u^*_{2j+1} = (x^1_{4j+3}, x^2_{4j+3}, \dots ,x^k_{4j+3}) 
        \end{align*}
        Note that the sequences $|v_n|$, $|u_n|$ belong to $l^2$. Also $u_n$ and $v_n$ are defined for all $n > 0$.
        We can rewrite the previous equations using the introduced vectors:
        \begin{align*}
            \Xi_{4j} &= a_{2j} \langle u_{2j}, v_{2j}\rangle + c_{2j} \langle v_{2j+1}, v_{2j}\rangle,\\
            \Xi_{4j + 1} &= -d_{2j} \langle v_{2j+1}, v_{2j}\rangle + b_{2j} \langle v_{2j+1}, u_{2j}\rangle,\\
            \Xi_{4j + 2} &= a_{2j+1} \langle v_{2j+1}, u_{2j+1} \rangle + c_{2j+1} \langle v_{2j+1}, v_{2j+2} \rangle,\\
            \Xi_{4j + 3} &= -d_{2j+1} \langle v_{2j+1}, v_{2j+2}\rangle + b_{2j+1} \langle u_{2j+1}, v_{2j+2} \rangle.
        \end{align*}
        Here the $\langle\cdot, \cdot\rangle$ denotes a standard scalar product in $\mathbb{R}^k$.
        \begin{note}
        Observe that in such setup we always have $\Xi_n = 0$ for any $n \leq 1$.
        \end{note}
        Since the vectors are in the $R^k$ we are able to reduce the system to
        \begin{align*}
            \Xi_{2j} &= a_{j} \langle u_{j}, v_{j} \rangle  + c_{j} \langle v_{j+1}, v_{j} \rangle,\\
            \Xi_{2j + 1} &= -d_{j} \langle v_{j+1}, v_{j} \rangle + b_{j} \langle v_{j+1}, u_{j}\rangle.
        \end{align*}
        We get that the existence of an operator $T$
        might be reduced to the existence of vectors $u_n$, $v_n$ in the $\mathbb{R}^k$ which
        respects the conditions given above.
        Given that the sequences of vectors $u_n$, $v_n$ lie in the $\mathbb{R}^k$ we might think of the scalar product as
        the usual product of the vector lengths and the cosinus of the angle between the vectors.
        \begin{prop}
            \label{k-dim-statement}
            If $\mu_n$ belongs to $l^1$ then it is possible to construct such vectors $u_n$ and $v_n$ that the equations above are true and for any $n > 1$ we have $\Xi_n = 1$.
        \end{prop}
        \begin{proof}
            The operator matrix is going to look very similar to one we built in the previous proposition.
            Again we are going to look at three possible values of the $\mu_n$.
            For each $n$ we are going to find $V_n = |v_n|$, $U_n = |u_n|$ and three angles:
            $\alpha_n$~--- is the angle between $v_n$ and $v_{n + 1}$, and
            $\beta_n$~--- is the angle between $v_n$ and $u_n$.
            $\gamma_n$~--- is the angle between $v_{n + 1}$ and $u_n$.
            Let $V_1$ be equal to one.
            We choose the $v^*_1$ vector such that $\langle v_1, v^*_1 \rangle = -1$ and set all
            the other $u^*_n$, $v^*_n$ to zero. That guarantees that the trace of the constructed operator (if it
            will fall within the trace class) is equal to $-1$.\\
            Within the construction we are going to set all the $V_n$ inductively. In order to do
            that we are going to define an auxiliary sequence $M_n \in l^2$ such that for any $n$ greater than zero
            $V_n \geq M_n$. On each step $n$ we are going to define $V_n$ and $M_{n+1}$.
            
            So let us start the construction with the setting $M_1 = 1$.\\
            \textbf{Case 1:} $\mu_n = 1/|a_n| + 1/|b_n|$\\
                Here we want to have $v_n$ orthogonal to $v_{n+1}$.
                Set
                \begin{align*}
                    V_n &:= \max\left(M_n, \frac{1}{\sqrt{\smash[b]{|a_n|}}}\right),\\
                    U_n &:= \left(\frac{1}{a_n^2 V_n^2} + \frac{1}{b_n^2 V_{n+1}^2}\right)^\frac{1}{2},\\
                    M_{n+1} &:= \frac{1}{\sqrt{\smash[b]{|b_n|}}}.
                \end{align*}
                \begin{prop}
                    The following inequalities are true:
                    \begin{align*}
                        M_{n+1} &\leq \sqrt{\mu_n},\\
                        U_n &\leq \sqrt{\mu_n},\\
                        V_n &\leq \max(\sqrt{\mu_n}, M_n).
                    \end{align*}
                    There exist such angles $\beta_n$, $\gamma_n$ that with the values $U_n$, $V_n$ defined like this the following is true:
                    \begin{align*}
                        \langle u_n, v_n \rangle &= 1/a_n,\\
                        \langle u_n, v_{n+1} \rangle &= 1/b_n,\\
                        \langle v_n, v_{n+1} \rangle &= 0.
                    \end{align*}
                \end{prop}
                \begin{proof}
                    First part of the proposition is a simple exercise.\\
                    Now we want to understand why there exist such $\beta_n$ and $\gamma_n$ that all the scalar products of
                    $v_n$, $v_{n+1}$, $u_n$ satisfy the conditions above.
                    Now observe that the chosen $U_n$, $1/(|a_n| V_n)$ and $1/(|b_n| V_{n+1})$ 
                    comprise a right triangle with $U_n$ as a hypotenuse.
                    As a result there always be such $\beta_n$ and $\gamma_n$ that
                    \begin{align*}
                        |U_n \cos{\beta_n}| &= \frac{1}{|a_n|V_n},\\
                        \left|U_n \cos{\gamma_n}\right| &= \left|U_n \sin{\beta_n}\right| = \frac{1}{|b_n|V_{n+1}}.
                    \end{align*}
                \end{proof}
            \noindent\textbf{Case 2:} $\mu_n = (1 + |a_n|)/|c_n|$\\
                Here we will have $u_n$ orthogonal to $v_n$.
                Assign:
                \begin{align*}
                    M_{n+1} &:= \max\biggl(\smash[b]{\frac{\sqrt{|a_n|}}{\sqrt{|c_n|}}}, \frac{1}{\sqrt{\smash[b]{|c_n|}}}\biggr),\\
                    V_n &:= \max\left(M_n, \frac{2}{\sqrt{\smash[b]{|c_n|}}}\right),\\
                    \alpha_n &:= \arccos{\frac{1}{c_n V_n V_{n+1}}},\\
                    \gamma_n &:= \frac{\pi}{2} \pm \alpha_n,\\
                    U_n &:= \frac{a_n}{c_n \cos{\gamma_n} V_{n+1}}.
                \end{align*}
                \begin{remark*}
                    We choose plus or minus in the expression of $\gamma_n$ in order to make the $a_n/(c_n \cos{\gamma_n})$ always positive.
                \end{remark*}
                \begin{prop}
                    The angle $\alpha_n$ is defined correctly and the following inequalities are true:
                    \begin{align*}
                        M_{n+1} &\leq \sqrt{\mu_n},\\
                        V_n &\leq \max(2\sqrt{\mu_n}, M_n),\\
                        U_n &\leq \sqrt{\mu_n}.
                    \end{align*}
                    With the values $U_n$, $V_n$, $\gamma_n$, $\alpha_n$ defined like this the following is true:
                    \begin{align*}
                        \langle u_n, v_n \rangle &= 0,\\
                        \langle u_n, v_{n+1} \rangle &= a_n/c_n,\\
                        \langle v_n, v_{n+1} \rangle &= 1/c_n.
                    \end{align*}
                \end{prop}
                \begin{proof}
                    Firstly $\alpha_n$ is defined correctly, since
                    $$
                    V_n V_{n+1} \geq V_n M_{n+1} \geq \frac{2}{\sqrt{\smash[b]{|c_n|}}} \frac{1}{\sqrt{\smash[b]{|c_n|}}}
                     = \frac{2}{|c_n|}
                    $$
                    which means that the absolute value of the arccosinus argument is always less than $1/2$. Now taking into
                    account that $|\cos{\alpha_n}|$ is less than or equal to $1/2$, we have that $|\cos{\gamma_n}| = |\sin{\alpha_n}|$ is always greater than $1/2$. It is the fact which will use later on in this proof.
                    
                    Trivially $M_{n+1} \leq \sqrt{\mu_n}$ and $V_n \leq \max(2\sqrt{\mu_n}, M_n)$. Next
                    $$
                        U_n = |U_n| \leq 2 \frac{|a_n|}{|c_n|} \frac{1}{V_{n+1}} \leq 2 \frac{\sqrt{|a_n|}}{\sqrt{|c_n|}} \leq \sqrt{\mu_n}.
                    $$
                    It is easy to notice that the angles and $U_n$ are chosen in such way that the scalar product conditions are 
                    satisfied.
                    Finally, we are able to lay out the vectors $v_n$, $u_n$ and $v_{n+1}$ with the prescribed angles 
                    since we have $\beta_n = \pi/2 = (\pi/2 \pm \alpha_n) \mp \alpha_n = \gamma_n \mp \alpha_n$,
                    where we choose the '$\mp$' sign accordingly to our choice in the expression of the $\gamma_n$ angle.
                \end{proof}
            \noindent\textbf{Case 3:} $\mu_n = (1 + |b_n|)/|d_n|$\\
                This case is almost identical to the previous one.\\
                Here we will have $u_n$ orthogonal to $v_{n+1}$.
                Assign:
                \begin{align*}
                    M_{n+1} &:= \max\biggl(\smash[t]{\frac{\sqrt{|b_n|}}{\sqrt{|d_n|}}}, \frac{1}{\sqrt{\smash[b]{|d_n|}}}\biggr),\\
                    V_n &:= \max\left(M_n, \frac{2}{\sqrt{\smash[b]{|d_n|}}}\right),\\
                    \alpha_n &:= \arccos{\frac{1}{-d_n V_n V_{n+1}}},\\
                    \beta_n &:= \frac{\pi}{2} \pm \alpha_n,\\
                    U_n &:= \frac{b_n}{d_n \cos{\beta_n} V_n}.
                \end{align*}
                \begin{remark*}
                    We choose plus or minus in the expression of $\beta_n$ in order to make the $b_n/(d_n \cos{\beta_n})$ always positive.
                \end{remark*}
                \begin{prop}
                    The angle $\alpha_n$ is defined correctly and the following inequalities are true:
                    \begin{align*}
                        M_{n+1} &\leq \sqrt{\mu_n},\\
                        V_n &\leq \max(2\sqrt{\mu_n}, M_n),\\
                        U_n &\leq \sqrt{\mu_n}.
                    \end{align*}
                    With the values $U_n$, $V_n$, $\beta_n$, $\alpha_n$ defined like this the following is true:
                    \begin{align*}
                        \langle u_n, v_n \rangle &= b_n/d_n,\\
                        \langle u_n, v_{n+1} \rangle &= 0,\\
                        \langle v_n, v_{n+1} \rangle &= -1/d_n.
                    \end{align*}
                \end{prop}
                \begin{proof}
                    Firstly $\alpha_n$ is defined correctly, since
                    $$
                    V_n V_{n+1} \geq V_n M_{n+1} \geq \frac{2}{\sqrt{\smash[b]{|d_n|}}} \frac{1}{\sqrt{\smash[b]{|d_n|}}}
                     = \frac{2}{|d_n|}
                    $$
                    which means that the absolute value of the arccosinus argument is always less than $1/2$. Now taking into
                    account that $|\cos{\alpha_n}|$ is less than or equal to $1/2$, we always have $|\cos{\beta_n}| > 1/2$.
                    
                    Trivially $M_{n+1} \leq \sqrt{\mu_n}$ and $V_n \leq \max(2\sqrt{\mu_n}, M_n)$. Next
                    $$
                        U_n = |U_n| \leq 2 \frac{|b_n|}{|d_n|} \frac{1}{V_n} \leq 2 \frac{\sqrt{|b_n|}}{\sqrt{|d_n|}} \leq \sqrt{\mu_n}.
                    $$
                    It is easy to notice that the angles and $U_n$ are chosen in such way that the scalar product conditions are 
                    satisfied.
                    Finally, we are able to lay out the vectors $v_n$, $u_n$ and $v_{n+1}$ with the prescribed angles 
                    since we have $\gamma_n = \pi/2 = (\pi/2 \pm \alpha_n) \mp \alpha_n = \beta_n \mp \alpha_n$,
                    where we choose the '$\mp$' sign accordingly to our choice in the expression of the $\beta_n$ angle.
                \end{proof}
            In each of three cases we guaranteed that $M_{n+1} \leq \sqrt{\mu_n}$ and
            that $V_n \leq \max(M_n, 2\sqrt{\mu_n})$. Hence we get that $V_n$ is bounded up to some constant by the $\max(\sqrt{\mu_{n-1}}, \sqrt{\mu_n})$. Due to all three propositions we also have an estimated bound for $U_n$: we always get
            $U_n \leq \sqrt{\mu_n}$. Thus the constructed vector lenghs sequences $V_n$ and $U_n$ belong to $l^2$.\\
            In the end let us choose such $v_1^*$ that $\langle v_1, v_1^*\rangle$ is equal to $-1$.
            It is possible since we earlier set $M_1 = 1$ which means that the vector $v_1$ is not trivial, because it has
            a length greater than zero.
            Additionally we set all the other $v^*_n$ and $u^*_n$ to zero in order to guarantee that for any $n > 1$ we have $\Xi_n$ equal to $1$. The trace of such operator is obviously equal to $-1$. The proposition is proven.
        \end{proof}
        The proof of the theorem is finished with the proof of the statements~\ref{inf-dim-statement} and~\ref{k-dim-statement}.
    \end{proof}
    
    
\medskip
% E-mail: aapyshkin@gmail.com
\bigskip
\begin {thebibliography}{20}
    \bibitem{azoff}
    E.~\!Azoff, H.~\!Shehada,
    \emph{Algebras generated by mutually orthogonal idempotent operators}.
    J. Oper. Theory, 29 (1993), 2, 249--267.
    \bibitem{bbb} 
    A. Baranov, Yu. Belov, A. Borichev,                                       
    \emph{Hereditary completeness for systems of exponentials and reproducing kernels},
    Adv. Math., 235 (2013), 1, 525--554.
    \bibitem{bbb1}
    A. Baranov, Yu. Belov, A. Borichev, 
    \emph{Spectral synthesis in de Branges spaces},
    Geom. Funct. Anal. (GAFA), 25 (2015), 2, 417--452.
    \bibitem{ad_preprint}
    A.D.~\!Baranov, D.V.~\!Yakubovich,
    \emph{Completeness and spectral synthesis of nonselfadjoint one-dimensional
    perturbations of selfadjoint operators}.
    arXiv:1212.5965 [math.FA]
    \bibitem{katavolos}
    A.~\!Katavolos, M.~\!Lambrou, M.~\!Papadakis,
    \emph{On some algebras diagonalized by $M$-bases of $\ell^2$}.
    Integr. Equat. Oper. Theory, 17 (1993), 1, 68--94.
    %\bibitem{wermer}
    %J.~\!Wermer,
    %\emph{On invariant subspaces of normal operators}.
    %Proc. Amer. Math. Soc., 3(1952), 2, 270--277.
    \bibitem{larson}
    D.~\!Larson, W.~\!Wogen,
    \emph{Reflexivity properties of $T\bigoplus0$}.
    J. Funct. Anal., 92 (1990), 448--467.
    %\bibitem{rotfeld}
    %В.В.~\!Пеллер,
    %\emph{Операторы Ганкеля и их приложения}.
    %Издательство РХД, Ижевск(2005).
    %N.K.~\!Nikol'skii,
    %\emph{Complete extensions of Volterra operators},
    %Izv. Akad. Nauk SSSR Ser. Mat 33(1969), 1349--1355. (Russian)

\end{thebibliography}
\vspace{1em}
\noindent{\bf Keywords:} complete minimal system, biorthogonal system, hereditary completeness, strong M-basis, summation method.

\end{document}
