\documentclass[12pt]{article}
\usepackage{cmap} % тоже для кодировки
%\usepackage[cp866]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc} % любая желаемая кодировка
\usepackage[english]{babel}
\usepackage[pdftex,unicode]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\usepackage{extsizes}
\usepackage{float}
\usepackage{bbold}
\usepackage{dsfont}
\usepackage{calc}
\usepackage{bm}
%\usepackage [a4paper,% other options: a3paper, a5paper, etc
%  left=3cm,
%  right=1.5cm,
%  top=2cm,
%  bottom=2cm,
%]{geometry}

\usepackage{tocloft}
%\renewcommand{\cfttoctitlefont}{\hspace{0.38\textwidth} \bfseries}
%\renewcommand{\cftbeforetoctitleskip}{-1em}
%\renewcommand{\cftaftertoctitle}{\mbox{}\hfill \\ \mbox{}\hfill{\footnotesize Стр.}\vspace{-.5em}}
%\providecommand{\cftchapfont}{\normalsize\bfseries \sectionname}
%\renewcommand{\cftsecfont}{\hspace{31pt}}
%\renewcommand{\cftsubsecfont}{\hspace{11pt}}
%\providecommand{\cftbeforechapskip}{1em}
%\renewcommand{\cftparskip}{-1mm}
%\renewcommand{\cftdotsep}{1}
%\setcounter{tocdepth}{2} % задать глубину оглавления - до subsection включительно

\usepackage{titlesec}
\sloppy
%\titleformat{\section}
%{\normalsize\bfseries}
%{\thesection}
%{1em}{}

%\titleformat{\subsection}
%{\normalsize\bfseries}
%{\thesubsection}
%{1em}{}

% Настройка вертикальных и горизонтальных отступов
%\titlespacing*{\chapter}{0pt}{-30pt}{8pt}
%\titlespacing*{\section}{\parindent}{*4}{*4}
%
%\linespread{1.3}

\newlength{\widecommentlength}
\setlength{\widecommentlength}{5in}
% \newcommand{\widecommentbox}[2]{\def#1##1{\strut\newline\noindent\colorbox{#2}{\linespread{1}\parbox{.95\textwidth}{\small ##1}}\newline}}
% \usepackage{pgfplots}
\newcommand{\widecommentbox}[3]{\def#1##1{\strut\newline\noindent\colorbox{#3}{\linespread{1}\parbox{.95\textwidth}{\small {\bf [#2]} ##1}}\newline}}
\def\commentsep{\noindent\dotfill}
\newcommand\inner[2]{\langle #1, #2 \rangle}

% To temporarily omit all comments, enable these two lines:
% \renewcommand{\widecommentbox}[3]{\def#1##1{}}
% \let\commentsep\relax

\widecommentbox{\alex}{Alex}{green!20!white}
\widecommentbox{\ad}{AD}{red!20!white}

\usepackage{enumitem}
\usepackage{setspace}

\makeatletter
%\@addtoreset{theorem}{section}
%\@addtoreset{lemma}{section}
\@addtoreset{prop}{section}
\makeatother

%\newcommand{\sectionbreak}{\clearpage}
\usepackage[square,numbers,sort&compress]{natbib}
\usepackage{mathtools}
\renewcommand{\bibnumfmt}[1]{#1.\hfill} % нумерация источников в самом списке - через точку
% \renewcommand{\bibsection}{\section*{Список литературы}} % заголовок специального раздела
\setlength{\bibsep}{0pt}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}%

%\titleformat{\section}[block]{\Large\bfseries\centering}{}{1em}{}
%\titleformat{\subsection}[block]{\large\bfseries\centering}{}{1em}{}
\renewcommand{\cal}[1]{\mathcal{#1}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\phi}{\varphi}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{prop_under_lemma}{Утверждение}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{corol}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem*{note}{Note}
\newtheorem{definition}{Definition}
\newtheorem{example}{Пример}
\newcommand\bigmatrixzero{\raisebox{-0.25\height}{\textnormal{\Huge 0}}}
\newcommand\bigzero{\makebox(10, 10){\text{\Huge 0}}}
\newcommand{\seq}[1]{\{{#1}_n\}_{n=1}^\infty}
\newcommand{\fsys}{\mathfrak{F}}
\newcommand{\fstarsys}{\mathfrak{F^{*}}}
\newcommand{\wt}{\mathrm{w}}
\newcommand{\wtp}{\mathrm{w}^{+}}
\newcommand{\flow}{\mathcal{F}}
\newcommand{\flowpos}{\mathcal{F}^{+}}
\newcommand{\flowposn}[1]{\mathcal{F}_{#1}^{+}}
\newcommand{\flowsgn}{\flow}
\newcommand{\source}{\mathit{source}}
\newcommand{\sink}{\mathit{sink}}
\renewcommand{\root}{\mathit{root}}
\newcommand{\scal}[2]{\langle {#1}, {#2} \rangle}
\newcommand{\net}{\Delta}
\newcommand{\onet}{\vec{\Delta}}

\numberwithin{remark}{section}
\numberwithin{theorem}{section}
\numberwithin{prop}{section}
\numberwithin{equation}{section}
\numberwithin{lemma}{section}
\numberwithin{prop_under_lemma}{lemma}

\begin{document}
% \title{Методы суммирования ряда Фурье \\
% относительно системы Азоффа--Шехада}

% \author{Алексей Пышкин\thanks{Работа поддержана грантом Президента РФ для государственной 
% поддержки молодых российских учёных -- докторов наук МД-5758.2015.1.}}
% \date{}

\section{<Simplified Azoff-shehada construction}
\section{B-class of the vector system}
  \subsection{From vector system to the countable network: linear summation problem}
    We are interested in the completeness properties of the minimal vector system $\fsys = \seq{f}$
      in a separable real [TODO complex] Hilbert space $\cal{H}$.
    Suppose that $\cal{H}$ has an orthonormal basis $\seq{e}$.
    Let $\fstarsys = \seq{f^*}$ be a biorthogonal vector system to the original system $\seq{f}$.
    \begin{definition}
        The vector system $\fsys$ belongs to the \textit{B-class} whenever the following conditions are satisfied:
        \begin{itemize}
            \item either $f_n = e_n$ or $f^*_n = e_n$ for any $n > 0$,
            \item $\inner{f_n}{e_n} = \inner{f^*_n}{e_n} = 1$ for any $n > 0$,
            \item $\inner{f_n}{e_k} = -\inner{f^*_k}{e_n}$ for any $n, k > 0$,
            \item the matrices $\{\inner{f_n}{e_k}\}$ and $\{\inner{f^*_n}{e_k}\}$ are both finite-band.
        \end{itemize}
    \end{definition}
    This class of vector systems is the subject of this text.
    \begin{prop}
        The definition given above guarantess the biorthogonality of the $f_n$ and $f^*_n$.
    \end{prop}
    \begin{proof}
        Straightforward check.
    \end{proof}
    \begin{remark}
        Note that the Azoff-Shehada example~\cite{azoff} lies in the B-class.
    \end{remark}
    
    B-class has an interesting property we are going to exploit later.
    \begin{prop}
        A vector system which belongs to the B-class could be associated with a 
        weighted bipartite graph $B(\{f_k\}) = (V, E, \wt)$, where $\wt$ is a
        real-valued function on the $V\times V$ such that $\wt(v, u) = -\wt(u, v)$.
        Also the inverse is true: for each bipartite graph $(V, E, \wt)$ for which the weight function
        $\wt$ satisifies the conditions above there is a vector system from the B-class.
    \end{prop}
    \begin{proof}
        For each index $l > 0$ such that $f^*_l = e_l$ we put a vertex $v_l$ in the first part of the bipartite graph.
        We will call this part from now on the \textit{left} part of the graph $B(\fsys)$.
        For any other index $r > 0$ we construct a vertex in the other part of the graph.
        Evidently, for such indices $r > 0$ the condition $f_r = e_r$ holds due to the definition of the B-class vector systems.
        The second part of the graph will be referred as the \textit{right} part of the graph $B(\fsys)$.
        We put an edge between two vertices $v_l$ and $v_r$ from the left and right parts respectively,
        whenever the scalar product $\inner{f_l}{e_r}$ is not zero.
        For such two vertices we have $\inner{f_l}{e_r}$ = $-\inner{f_r}{e_l}$.
        Denote by $\wt_{lr}$ the expression $\wt_{lr} = \inner{f_l}{e_r}^{-1}$.
        Obviously $\wt_{lr} = -\wt_{rl}$.
        //todo inverse
    \end{proof}
    We are going to find the necessary and sufficient condition for the $k$-completeness property for the B-class vector system.
    In order to pursue that we need to investigate the conditions, under which the B-class system admits a linear summation method.
    First of all we intend to demonstrate an elegant reformulation of the linear summation method existence problem for
      the B-class vector systems.
    Recall that there exists a linear summation method for the system $\seq{f}$
      iff there is no trace class operator $T: \cal{H} \to \cal{H}$ with the trace equal to $1$,
      such that $\inner{Tf_n}{f_n^*} = 0$ for any $n$.
    Suppose that there is such an operator $T$ such that $\inner{Tf_n}{f_n^*} = 0$ for any $n$.
    There are two cases: either $f^*_n = e_n$ or $f_n = e_n$.
    In the first case the condition $\inner{Tf_n}{f_n^*} = 0$ turns into
    \begin{equation}
        \label{left-eqn}
        \sum_j T_{nj} \inner{f_n}{e_j} = 0,
    \end{equation}
    and in the second case it is equivalent to 
    \begin{equation}
        \label{right-eqn}
        \sum_j T_{jn} \inner{f^*_n}{e_j} = 0.
    \end{equation}
    Now consider a real function $\flowsgn(T): E \to \mathbb{R}$ 
    on the edges of the graph $B(\fsys)$.
    \begin{align*}
        \flowsgn(v_l, v_r) := T_{lr} \inner{f_l}{e_r} = T_{lr} \wt_{lr}^{-1},\\
        \flowsgn(v_r, v_l) := T_{lr} \inner{f^*_r}{e_l} = T_{lr} \wt_{rl}^{-1}.
    \end{align*}
    Observe that $\flowsgn(T)$ is a skew-symmetric function.
    Moreover, two equalities~\eqref{left-eqn} and~\eqref{right-eqn} correspond to the \textit{left} and \textit{right}
      parts of the bipartite graph $B(\fsys)$ respectively.
    It follows that the condition $\inner{Tf_n}{f_n^*} = 0$ could be reduced to the simpler one:
    \begin{equation}
      \label{almost-flow-eqn}
      \sum_{u \in V} \flowsgn(v, u) + T_{vv} = 0
    \end{equation}
      for each vertex $v$ in the graph $B(\fsys)$.
    \begin{remark}
      Notice how the function $\flow$ defined on the graph $B(\fsys)$ resembles
      a \emph{flow} defined on the edges of the $B(\fsys)$ graph.
      One might also see that the equation~\eqref{almost-flow-eqn} describes the total flow (sum of the outgoing flow and incoming flow) 
      for each vertex $v$ in the graph $B(\fsys)$
      In order to formalize this observation we are going to build a flow function
      after a few changes are made to the graph $B(\fsys)$.
    \end{remark}
    Befory we proceed we are going to introduce a few basic definitions.
    \begin{definition}
        \emph{Network} $\net$ is a quadruple $(G, \wtp, s, t)$, where $G = (V, E)$ is weighted graph 
        with a positive weight function $\wtp$ on $E$ and two vertices $s, t \in V$, which
        we will call $\source$ and $\sink$ of the network respectively.
    \end{definition}
    \begin{definition}
        Look at the skew-symmetric function $\flow: V \times V \to R$.
        The function $\flow$ will be called a \emph{pseudo-flow}.
    \end{definition}
    \begin{definition}
      Let $\net = (G, \wtp, s, t)$ be a network, and $\flow$ be a pseudo-flow function.
      For each vertex $v \in V$ we take $IN(v)$ as the set of incoming edges
      and $OUT(v)$ as the set of outgoing edges in the graph $G$.
      Also denote by $d_{+}(v)$ the sum of the flows \emph{leaving} the node $v$ and by
      $d_{-}(v)$ the sum of the flows \emph{entering} $v$:
      \begin{align*}
          d_{+}(v) = \sum_{u \in OUT(v)} \flow(vu),\\
          d_{-}(v) = \sum_{u \in IN(v)} \flow(uv).
      \end{align*}
      Also let $d(v)$ be equal to $d_{+}(v) - d_{-}(v)$. Sometimes we will refer to this value as
      a \emph{total flow} of the vertex $v$.
      The vertex is called $deficient$ iff $d(v)$ is less than zero,
      $active$ iff $d(v)$ is greater than zero and
      $preserving$ iff $d(v)$ is precisely zero, meaning that the total incoming flow
      is equal to the total outcoming flow of the vertex $v$.
    \end{definition}
    \begin{definition}
      Given a network $\net = (V, E, \wtp, s, t)$ and a pseudo-flow function $\flow$ we will name $\flow$ a
        \emph{flow} function iff for any vertex $v \in V \setminus \{s,t\}$ the total flow is zero: $d(v) = 0$.
    \end{definition}
    \begin{definition}
      For a network $\net = (G, \wtp, s, t)$ and a flow $\flow$ we will say,
      that the flow $\flow$ is \emph{preserving} if $d(\sink) = -d(\source)$ vertex.
    \end{definition}
    \begin{note}
      In simple words this property suggests
        that the total flow coming out of the $\source$ is equal to the total flow
        coming into the $\sink$.
    \end{note}
    \begin{definition}
      \emph{Oriented network} $\onet$ is a quadruple $(\vec{G}, \wtp, s, t)$, where
        $\vec{G}$ is an oriented weighted graph with 
        a positive weight function $\wtp$ on $\vec{E}$ and two vertices $s, t \in V$, which
        we will call $\source$ and $\sink$ of the oriented network respectively.
    \end{definition}
    \begin{note}
      The flow functions could be defined on the oriented networks in the similar manner
      we defined on the non-oriented networks.
    \end{note}
    Suppose $\net = (V, E, \wtp, s, t)$ is a network and $\flow$ is a skew-symmetric function defined
    on $\net$.
    Naturally, we might instead consider an oriented network $\onet = (V, \vec{E}, \wtp_{*}, s, t)$ and a 
      positive flow function $\flowpos: \vec{E} \to \mathbb{R}^{+}$, where
      the set of vertices $V$, $\source$ and $\sink$ vertices are the same.
    The weight function $\wtp_{*}$ will be also positive but will be redefined since the edges set $\vec{E}$ is a different one.
    First we provide a direction of the edge $(uv)$ is as follows:
    \begin{itemize}
      \item $(uv) \in \vec{E}$ if $\flow(u, v) > 0$,
      \item $(vu) \in \vec{E}$ if $\flow(u, v) < 0$.
    \end{itemize}
    This way only one of the edges $(uv)$ and $(vu)$ is present in the graph $\vec{G} = (V, \vec{E})$.
    Also we set up the weight and the positive flow function $\flowpos: E \to \mathbb{R}^{+}$ so that
    for any edge $\vec{e} = (uv) \in \vec{E}$
    \begin{align*}
      \flowpos(uv) = |\flow(u,v)|,\\
      \wtp_{*}(uv) = \wtp(u,v).
    \end{align*}
    \begin{definition}
      The oriented network $\onet$ is called an \emph{orientation} of the network $\net$.
      For the positive flow function $\flowpos$ we will use the same term:
        $\flowpos$ is an \emph{orientation} of the flow $\flow$.
    \end{definition}
    \begin{remark}
      As shown, the flow functions on the networks and the positive flow functions on the oriented networks
        are interchangable and describe the same object.
      The notion of the direction of the positive flow over the particular edge $(uv)$ in the oriented network
        is incorporated into the sign of the flow function on the same edge $(uv)$ in the non-oriented network.
    \end{remark}
    \begin{definition}
      Consider a network $\net = (G, \wtp, s, t)$ such that
        the degree of each vertex in $V \setminus \{s, t\} $ is finite,
        and the weight of each edge incident to $\source$ or to $\sink$ is equal to one.
      We will call such network a \emph{B-class network}.
    \end{definition}
    
    Now we are ready to continue with the linear summation method problem.
    Our aim is to build up a network from the graph $B(\fsys)$.
    We plan to construct the network $\net(\fsys) = (V, E, \wtp, s, t)$,
    positive weight function $\wtp: E \to R^{+}$ and a real skew-symmeteric flow function $\flow: V \times V \to R$.
    Firstly we consider the vertices and the edges of the graph $B(\fsys)$.
    All the vertices in the graph $B(\fsys)$ we incorporate into the network $\net(\fsys)$.
    The vertex set of the new graph is a superset of vertices of the graph $B(\fsys)$,
    so we will continue referring to the vertices as the vertices from 
    the \textit{left} part of the graph and the vertices from the \textit{right} part of the graph.
    The network $\net(\fsys)$ will contain all the edges from the graph $B(\fsys)$ as well.
    For each of these edges we are going to assign the flow function $\flow$ and
    establish a new positive weight function $\wtp$.
    Each edge $e = (v_l v_r)$ which belongs to the graph $B(\fsys)$ we include in the graph of $\net(\fsys)$.
    We set the flow and the weight functions on the edge $e$:
    \begin{align*}
      &\flow(v_l, v_r) = \flowsgn(v_r, v_l) = T_{lr} \inner{f^*_r}{e_l},\\
      &\wtp(e) = \left| \wt(e) \right|.
    \end{align*}
    Evidently on this kind of edges the flow functions $\flowsgn$ and $\flow$ agree.
    
    Now we add the $\source$ vertex $s$ and the $\sink$ vertex $t$ to the constructed graph.
    Suppose we have enumerated the vertices from the left part of the graph, for instance $\seq{l}$.
    Then for each $k > 0$ we connect the $\source$ vertex and the $l_k$ vertex with
      the edge $e_k = (s l_k)$, and put a flow:
    \begin{align*}
      &\flow(l_k, s) = T_{l_k l_k},\\
      &\flow(s, l_k) = -T_{l_k l_k}.
    \end{align*}
    We have the weight equal to one for such edges: $\wtp(e) = 1$.
    As we added the edges ${e_k}$, the flow became \emph{preserved} at each vertex of the left part of the network $\net(\fsys)$.
    In the similar manner we will transform the right part of the graph.
    Assuming we have the vertices from the right part of the graph $B(\fsys)$ in the form of the sequence $\seq{r}$,
      for each $k > 0$ we connect the $r_k$ vertex to the $\sink$ with the edge $e'_k=(r_k t)$
      and set the flow equal to:
    \begin{align*}
      &\flow(r_k, t) = T_{r_k r_k},\\
      &\flow(t, r_k) = -T_{r_k r_k}.
    \end{align*}
    For these edges we put the weight equal to one as well.
    The network $\net(\fsys)$ is built up, and it is a B-class network.
    The defined function $\flow$ is a flow, since each of the vertex preserves
      the flow due to the equation~\eqref{almost-flow-eqn}.
    %\begin{remark}
      %Sometimes it might be useful to consider slightly different definition of the network.
      %Instead of studying an oriented graph with a positive flow function on it we might consider
      %an undirected graph with a skew-symmetric flow function on the edges.
      %Intuitively, the direction of the flow could be expressed either via the direction of the edge
      %or with the sign of the flow function, and these two definitions describe the same objects.
      %Formally there is a isomophism between these two types of objects.
    %\end{remark}
    %\begin{definition}
      %Let $\net=(V,E,\flow)$ be a network -- an oriented graph with a positive flow function $\flow$ on it.
      %Consider an undirected graph $G=(V,E')$ such that $e=(vu)$ belongs to $E'$ iff
      %either $(vu) \in E$ or $(uv) \in E$.
      %Such undirected graph we will call the \it{frame} of the network $\net$.
      %Also define a function $\flowsgn$ on the $V \times V$:
      %\begin{align*}
        %\flowsgn(vu) = \flow(vu), \quad \text{if edge } (vu) \in E,\\
        %\flowsgn(vu) = -\flow(vu), \quad \text{if edge } (uv) \in E.
      %\end{align*}
      %Such function we will call \it{sign-flow} function.
    %\end{definition}
    %Regard the orientation $\onet = (\vec{B}(\fsys), \wtp, s, t)$ of the established network $\net(\fsys)$.
    \begin{remark}
      Pay your attention to the fact that the network $\net(\fsys)$
      depends only on the biorthogonal system $\fsys$ and not on the operator $T$.
      Only the constructed flow function $flow$ depends on the operator $T$.
    \end{remark}
    What can one say about the total flow in each of the vertex of the constructed network $\net(\fsys)$?
    Due to the the trick we performde the total flow became zero in each of the vertices from the \textit{left} and the \textit{right} parts.
    The total flow $d(\source)$ in the $\source$ vertex is equal to the $\sum T_{l_k l_k}$, and the total flow in the
    $\sink$ vertex is now equal to the $\sum T_{r_k r_k}$.
    Now one can see that the network $\net(\fsys)$ preserves the flow if and only if the trace of the operator $T$ is equal to
    zero.
    \begin{definition}
      Consider a weighted network $\net = (V, E, \flow, \wtp)$.
      We introduce a non-negative function $\|\flow\|$ which we call the \emph{mass} of the flow $\flow$.
      $$
        \|\flow\| = \sum_{e \in E} \flow(e) \wtp(e).
      $$
      We allow it to be equal $+\infty$ as well.
    \end{definition}
    \begin{theorem}
      \label{thm-graph-eq}
        Let $\net(\fsys) = (B(\fsys, \wtp, s, t)$ be a B-class network constructed
          from the B-class system $\fsys = \seq{f}$.
        Then $\fsys$ admits a linear summation method if and only if for any 
          flow function $\flow$ defined on $\net(\fsys)$ such that
          the mass of the flow $\flow$ is finite,
          the network $\net$ \emph{preserves} the flow $\flow$.
    \end{theorem}
    \begin{proof}
      First we briefly repeat the sufficiency proof we studied earlier.
      Suppose there is no summation method for the system $\seq{f}$.
      As we said before, it implies that there exists a trace class operator $T : \cal{H} \to \cal{H}$ with
      the trace equal to one, such that $\inner{Tf_n}{f_n^*} = 0$ for any $n$.
      Using the operator $T$, we were able to define a flow function $\flow(T)$ on the network
      $\net(\fsys)$.
      One thing to check is that the flow we constructed has a finite mass.
      It easy follows from the condition that the operator matrix is finite band and has a finite trace.
      Finally, one can acknowledge the fact that this network does NOT preserve the flow, since
      $$
        d(s) + d(t) = \sum T_{l_k l_k} + \sum T_{r_k r_k} = Tr(T) = 1,
      $$
      thus $d(\sink) \neq -d(\source)$.
      We arrived to contradiction and the sufficiency is proved.

      Now suppose that the system $\fsys$ admits a linear summation method.
      Assume there is also a flow $\flow$ on the network $\net(\fsys)$ such that $d(\sink) + d(\source) \neq 0$.
      Now we are going to construct the operator $T$ with non-zero trace, which annihilates all the rank-one
      operators $f_n \otimes f^*_n$, thus coming to the contradiction with our initial assumption.
      First consider the edges incident to the $\source$ vertex.
      Having all the vertices already enumerated we will consider the vertices in the left part $v_{l_k}$.
      Recall that each vertex $v_i$ (except $\source$ and $\sink$) matches to the basis element $e_i$.
      Assign the diagonal elements of $T$:
      $$
        T_{l_k l_k} := \flow(l_k, \source) \wtp(l_k l_k) = \flow(l_k, \source) \quad k > 0.
      $$
      Now we repeat the same procedure for the right part of the graph $\net(\fsys)$, thus having
      $$
        T_{r_k r_k} := \flow(r_k, \sink) \wtp(r_k r_k) = \flow(r_k, \sink) \quad k > 0.
      $$
      Now consider two connected vertices $v_l$, $v_r$ from the left and right part
      respectively. Assign
      $$
        T_{lr} := \flowsgn(v_l, v_r) * \wtp(v_l, v_r).
      $$
      Let all the other matrix elements of the operator matrix $T_{ij}$ be zero.
      Then observe that we got a finite-band operator matrix $T_{ij}$ with the sum of diagonal elements
      equal to one.
      Due to the finite mass of the flow $\flow$ the $T_{ij}$ is an absolute summable sequence, since the 
        $l^1$-norm of $T_{ij}$ is precisely the mass of $\flow$.
      Then the operator $T$ we obtained is compact.
      It acquires a finite non-zero trace, since $Tr(T) = d(\sink) + d(\source)$ which is not zero
        by the assumption we made.
      Due to the fact, that the flow was preserved at each vertex of the network $\net(\fsys)$,
        we see that the operator $T$ annihilates all the rank one operators $f_n \otimes f^*_n$,
        which leads to a contradiction.
    \end{proof}
    
  \subsection{B-class networks characterisation}
    Due to the theorem~\ref{thm-graph-eq} we intend to analyze the flows on the B-class network
      $\net(\fsys)$ in order to understand the conditions under which a linear
      summation method exists for $\fsys$.
    In this section we are not going to address the Hilbert space setup at all,
      instead we will use only the abstract objects of the graph theory
      we used throughout the previous section.
    Also it will be more convenient for us to study \emph{oriented} networks in this section unlike we
      did in the previous one.
    Remember some other notions from the graph theory:
    \begin{definition}
      Let $G$ be an oriented graph.
      A sequence of vertices $\{v_k\}_{k=1}^N$ is called a \emph{cycle} if 
    \end{definition}
    \begin{definition}
      Let $G$ be a oriented graph.
      A sequence of vertices $\{v_k\}_{k=1}^N$ is called a \emph{simple cycle} if 
    \end{definition}
    \begin{definition}
      Let $G = (V, E)$ be an infinite (oriented) graph.
      An infinite path $r = \seq(v)$ is called a \it{ray} iff
      $r$ is a path, meaning there is an edge $e = (v_k v_{k+1})$ for each $k > 0$ in $G$.
    \end{definition}

    The main result of this section is a following
    \begin{theorem}
      Let $\onet = (\vec{G}, \wtp, s, t)$ be an oriented B-class network.
      Then there exist such a positive flow function $\flowpos: \vec{E} \to R^{+}$ that
      \begin{itemize}
        \item the flow $\flowpos$ has a finite mass,
        \item the network $\onet$ does not preserve the flow,
      \end{itemize}
        if and only if there exists a ray $r = \seq{v}$ ($=sv_2v_3\dots$)
        in the network $\net$, which weight is bounded:
        $$
          \sum_{k=1}^\infty \wtp(v_k v_{k+1}) < \infty.
        $$
    \end{theorem}
    \begin{proof}
      Without loss of generality we might assume that $d(\sink) + d(\source)$ is precisely one.
      In order to make our proof easier we merge the $\source$ vertex with the $\sink$ vertex and call this a
        new $\source$ vertex.
      This way we obtain a network with a single source such that $d(\source) = 1$.
      Formally we are left without the $\sink$ vertex but we are going to choose any vertex $v \in V$ which
        is not equal to the new $\source$ and declare this $v$ a new $\sink$ vertex.
      All the vertices in the graph still preserve the flow after this procedure, except the $\source$ vertex
        which is now \emph{active} since its total flow $d(\source)$ equal to one.

        the existence of the ray with a finite total weight
        in the transformed network is equivalent to the existence of such ray in the original network.
      //todo
      \noindent\textit{Sufficiency}
      Suppose that there is a ray of a finite total weight. 
      In that case we set the resulting flow function $\flowpos(v_k v_{k+1})$ to one, creating an elusive flow
        from the $\source$ to infinity. All the other edges will not carry any flow, hence the flow is preserved
        at each vertex but not preserved by the network $\onet$.

      \noindent\textit{Necessity}
      Now let us turn to the more difficult part.
      In this part we are going to view this network as a rooted graph with the root at the $\source$ vertex.
      Consequently sometimes we will employ the term $\root$.
      For a start we get rid of all the flow cycles in the oriented graph $G$.

      Namely, we will reason by the transfinite induction and at each step $n \in \mathbb{N}$ construct a subnetwork
        $\onet_n = (\vec{G}_n, \wtp, \root, \root)$ and a subflow $\flowposn{n}$, namely such that for each $n$ greater than zero 
        $\flowposn{n}$ is bounded above by the original flow: $\flowposn{n}(e) \leq \flowpos(e)$ for any $e \in E_n$.
      Also we require that $\flowposn{n+1}(e) \leq \flowposn{n}(e)$.
      At the induction step $n$ we consider the subnetwork $\vec{G}_n = (V_n, \vec{E}_n)$, where the vertices
        $V_n = {v_k}_{k=1}^n$ and the edges $E_n$ is the edges from $\vec{E}$ incident to the $V_n$.
      The base is $n = 1$ is trivial: the network $\onet_1$ contains only the root vertex.
      Suppose that for the finite network $\onet_n$ and a flow function $\flowposn{n}: \vec{E}_n \to R^{+}$ 
        all the conditions above are satisfied.
      Let $\cal{G}_0(e)$ be a flow function on the graph $\vec{G}_{n+1}$.
      Initially put 
      \begin{equation*}
        \cal{G}_0(e) =
        \begin{cases}
          \flowposn{n}(e) \quad \text{if } e \in \vec{G}_n,\\
          \flowpos(e) \quad \text{otherwise}.
        \end{cases}
      \end{equation*}
      Let there be a simple cycle $C$ in the graph $\vec{G}_n$ such that each edge $e \in C$ carries
        a strictly positive flow.
      At this point we can decrease the flow $\cal{G}_0$ on the edges of $C$ in such a way, that the total mass
        of the flow will not increase, all the other condition will stay intact.
      Take the minimal value flow function attains on the edges of the cycle: $\displaystyle\min(C) = \min_{e \in C}\left(\cal{G}_0(e)\right)$.
      Consider a adjusted flow:
      \begin{equation*}
        \cal{G}_1(e) =
        \begin{cases}
          \cal{G}_0(e) - \min(C) \quad \text{if } e \in C,\\
          \cal{G}_0(e) \quad \text{otherwise}.
        \end{cases}
      \end{equation*}
      Since $C$ is a cycle, the $\cal{G}_1$ is indeed a flow function.
      After this positive flow cycle disappears, leaving us with a strictly lesser flow than we had before this step:
      \begin{align*}
        \cal{G}_1(e) \leq \flowpos(e) \quad \forall e \in \vec{G}_{n+1},\\
        \cal{G}_1(e) \leq \cal{G}_0(e) \quad \forall e \in \vec{G}_n.
      \end{align*}
      In this manner we remove all the simple positive-flow cycles from the finite graph $\vec{G}_n$. 
      It is achievable since at each step we turn at least one edge into a zero-flow edge.
      Put $\flowposn{w+1}$ equal to the flow function $\cal{G}_K$ we got after we remove $K$ cycles from the flow network $\onet_{n+1}$.
      We have
      \begin{align*}
        \flowposn{n+1}(e) \leq \flowpos(e) \quad \forall e \in \vec{G}_{n+1},\\
        \flowposn{n+1}(e) \leq \flowpos{n}(e) \quad \forall e \in \vec{G}_n.
      \end{align*}

      %Firstly we assume that all the edges carry a non-zero flow.
      %Otherwise we are able to consider a subnetwork without all zero-flow-edges and prove the implication for the
        %subnetwork.
    \end{proof}
    \subsection{Application of the B-class networks characterisation}
    %----------------------------------------------------------------------------------------
    \pagebreak
    \begin{prop}
        The given system is an $M$-basis if the coefficients satisfy the restrictions above.
    \end{prop}
    \begin{proof}
        The equality $c_n + d_n = a_n b_n$ guarantees the bi\-orthogonality,
        while the completeness of the $\{f_n\}$ and $\{f_n^*\}$ is
        easy to check.
    \end{proof}
    
    \begin{theorem}
        The given system is NOT $k$-complete for any $k > 1$ iff the sequence
        $$
            \mu_n = \min(1/|a_n| + 1/|b_n|, (1 + |b_n|)/|d_n|, (1 + |a_n|)/|c_n|)
        $$ belongs to $l^1$.
    \end{theorem}
    \begin{proof}
        Let us consider a $k$-dimensional operator $T$ such that 
        $Tr(TR) = 0$ for each $R \in \cal{R}_1(\cal{A})$ which essentially means that
        $\inner{Tf_n}{f_n^*} = 0$ for any $n$. 
        Notice that the partial sums of the fourier series for the given system are somehow close to the
        partial sums of the canonical fourier series (using the orthonormal basis $e_n$). Define
        $$
          \Xi_n := \sum_1^n \inner{Tf_s}{f_s^*} - \sum_1^n \inner{Te_s}{e_s} = -\sum_1^n \inner{Te_s}{e_s}.
        $$
        where the $\inner{\cdot}{\cdot}$ denotes a standard scalar product in $\cal{H}$.
        These residuals has also a concise form:
        \begin{align*}
            \Xi_{4j} = a_{2j} T_{4j+1, 4j} + c_{2j} T_{4j+2, 4j},\\
            \Xi_{4j + 1} = -d_{2j} T_{4j+2, 4j} + b_{2j} T_{4j+2, 4j+1},\\
            \Xi_{4j + 2} = a_{2j+1} T_{4j+2, 4j+3} + c_{2j+1} T_{4j+2, 4j+4},\\
            \Xi_{4j + 3} = -d_{2j+1} T_{4j+2, 4j+4} + b_{2j+1} T_{4j+3, 4j+4},
        \end{align*}
        having $T_{ij}$ equal to the $\inner{Te_j}{e_i}$.
        \begin{prop}
            \label{inf-dim-statement}
            There exists an operator $T$ with trace equal to $1$ such that
            $\inner{Tf_n}{f_n^*} = 0$ for any $n$ if and only if the
            sequence from the statement of the theorem $\mu_n$ belong to $l^1$.
        \end{prop}
        \begin{proof}
            Assume $\mu_n$ is a $l^1$ sequence. We will construct a required operator $T$.
            Let $T_{11}$ be equal to $-1$, and $T_{jj}$ be equal to zero for any other $j$.
            Then for each $n$ we have three cases:
            1. Suppose $\mu_n = 1/|a_n| + 1/|b_n|$. Then if $n$ is $2j$ for some integer $j$, then
            let:
            \begin{align*}
                T_{4j+1,4j}&=1/a_n & \quad T_{4j+2,4j} = 0,\\
                T_{4j+2,4j+1}&=1/b_n.
            \end{align*}
            That guarantees an equality $\Xi_{4j} = \Xi_{4j+1} = 1$.
            If $n$ is $2j+1$ then let:
            \begin{align*}
                T_{4j+2,4j+3}&=1/a_n & \quad T_{4j+2,4j+4} = 0,\\
                T_{4j+3,4j+4}&=1/b_n,
            \end{align*}
            which provides an equality $\Xi_{4j+2} = \Xi_{4j+3} = 1$.
            2. Consider the case when $\mu_n$ is equal to $(1 + |b_n|)/|d_n|$. 
            Suppose $n = 2j$ for some natural $j$.
            Here we assign
            \begin{align*}
                T_{4j+1,4j} &= b_{2j}/d_{2j} & \quad T_{4j+2,4j} = -1/d_{2j},\\
                T_{4j+2,4j+1} &= 0.
            \end{align*}
            The case $n = 2j + 1$ is essentially the same.
            3. Consider the case when $\mu_n$ is equal to $(1 + |a_n|)/|c_n|$. 
            Suppose $n = 2j + 1$ for some natural $j$.
            Here we assign
            \begin{align*}
                T_{4j+2,4j+3} &= 0 & \quad T_{4j+2,4j+4} = 1/c_{2j+1},\\
                T_{4j+3,4j+4} &= a_{2j+1}/c_{2j+1}.
            \end{align*}
            The case $n = 2j$ is essentially the same.
            All the other entries $T_{ij}$ we are setting to zero.
            The constructed operator $T$ obviously belongs to the trace class (since all the non-zero elements are summable 
            on the assumption that $\mu_n$ is a $l^1$ sequence).
            These assignments ensure that all the $\Xi_n$ are equal to $1$ for any $n > 1$.
            Since the trace of the operator $T$ is equal to
            $1$ and it annihilates all the rank one operators $f_n \otimes f^*_n$, the sufficiency is proved.
            \medskip\\
            Now assume that there exists a trace class operator $T$ with all the properties we require.
            Then obviously all the $T_{nn}, T_{n, n+1}, T_{n, n+2}$ elements are summable (simple property of
            a trace class operator matrix).
            As we know $\Xi_n$ tends to $1$ since the trace of the operator is $1$. Let us consider a sum
            $|T_{4j+1, 4j}| + |T_{4j+2,4j}| + |T_{4j+2,4j+1}|$. It depends linearly on $T_{4j+2, 4j}$ and the minimum
            is obtained on the boundary of the domain. The minimum value is greater than $\mu_{2j}/2$ whenever $j$ is
            sufficiently large.
            The second pair of equalities gives out the minimum value greater than $\mu_{2j+1}/2$ when $j$ is large,
            which shows that the stated condition is necessary for the existence of operator $T$.
        \end{proof}
        Now consider the case of the $k$-dimensional operator $T$.
        Let us put the operator $T$ as a sum of $k$ rank one operators:
        $$
            T = \sum_1^k y^s \otimes x^s,
        $$
        where $x^s, y^s \in \cal{H}$
        Therefore $T_{ij} = \sum {y^s_j x^s_i}$.
        Let us define vectors $v_n$ and $u_n$ which lie within $\mathbb{R}^k$ as follows:
        \begin{align*}
            v_{2j} &= (y^1_{4j}, y^2_{4j}, \dots ,y^k_{4j}) \quad
            &v^*_{2j} = (x^1_{4j}, x^2_{4j}, \dots ,x^k_{4j}) \\
            v_{2j+1} &= (x^1_{4j+2}, x^2_{4j+2}, \dots ,x^k_{4j+2}) \quad
            &v^*_{2j+1} = (y^1_{4j+2}, y^2_{4j+2}, \dots ,y^k_{4j+2}) \\
            u_{2j} &= (x^1_{4j+1}, x^2_{4j+1}, \dots ,x^k_{4j+1}) \quad
            &u^*_{2j} = (y^1_{4j+1}, y^2_{4j+1}, \dots ,y^k_{4j+1}) \\
            u_{2j+1} &= (y^1_{4j+3}, y^2_{4j+3}, \dots ,y^k_{4j+3}) \quad
            &u^*_{2j+1} = (x^1_{4j+3}, x^2_{4j+3}, \dots ,x^k_{4j+3}) 
        \end{align*}
        Note that the sequences $|v_n|$, $|u_n|$ belong to $l^2$. Also $u_n$ and $v_n$ are defined for all $n > 0$.
        We can rewrite the previous equations using the introduced vectors:
        \begin{align*}
            \Xi_{4j} &= a_{2j} \inner{u_{2j}}{v_{2j}} + c_{2j} \inner{v_{2j+1}}{v_{2j}},\\
            \Xi_{4j + 1} &= -d_{2j} \inner{v_{2j+1}}{v_{2j}} + b_{2j} \inner{v_{2j+1}}{u_{2j}},\\
            \Xi_{4j + 2} &= a_{2j+1} \inner{v_{2j+1}}{u_{2j+1}} + c_{2j+1} \inner{v_{2j+1}}{v_{2j+2}},\\
            \Xi_{4j + 3} &= -d_{2j+1} \inner{v_{2j+1}}{v_{2j+2}} + b_{2j+1} \inner{u_{2j+1}}{v_{2j+2}}.
        \end{align*}
        Here the $\inner{\cdot}{\cdot}$ denotes a standard scalar product in $\mathbb{R}^k$.
        \begin{note}
        Observe that in such setup we always have $\Xi_n = 0$ for any $n \leq 1$.
        \end{note}
        Since the vectors are in the $R^k$ we are able to reduce the system to
        \begin{align*}
            \Xi_{2j} &= a_{j} \inner{u_{j}}{v_{j}}  + c_{j} \inner{v_{j+1}}{v_{j}},\\
            \Xi_{2j + 1} &= -d_{j} \inner{v_{j+1}}{v_{j}} + b_{j} \inner{v_{j+1}}{u_{j}}.
        \end{align*}
        We get that the existence of an operator $T$
        might be reduced to the existence of vectors $u_n$, $v_n$ in the $\mathbb{R}^k$ which
        respects the conditions given above.
        Given that the sequences of vectors $u_n$, $v_n$ lie in the $\mathbb{R}^k$ we might think of the scalar product as
        the usual product of the vector lengths and the cosinus of the angle between the vectors.
        \begin{prop}
            \label{k-dim-statement}
            If $\mu_n$ belongs to $l^1$ then it is possible to construct such vectors $u_n$ and $v_n$ that the equations above are true and for any $n > 1$ we have $\Xi_n = 1$.
        \end{prop}
        \begin{proof}
            The operator matrix is going to look very similar to one we built in the previous proposition.
            Again we are going to look at three possible values of the $\mu_n$.
            For each $n$ we are going to find $V_n = |v_n|$, $U_n = |u_n|$ and three angles:
            $\alpha_n$~--- is the angle between $v_n$ and $v_{n + 1}$, and
            $\beta_n$~--- is the angle between $v_n$ and $u_n$.
            $\gamma_n$~--- is the angle between $v_{n + 1}$ and $u_n$.
            Let $V_1$ be equal to one.
            We choose the $v^*_1$ vector such that $\inner{v_1}{v^*_1} = -1$ and set all
            the other $u^*_n$, $v^*_n$ to zero. That guarantees that the trace of the constructed operator (if it
            will fall within the trace class) is equal to $-1$.\\
            Within the construction we are going to set all the $V_n$ inductively. In order to do
            that we are going to define an auxiliary sequence $M_n \in l^2$ such that for any $n$ greater than zero
            $V_n \geq M_n$. On each step $n$ we are going to define $V_n$ and $M_{n+1}$.
            
            So let us start the construction with the setting $M_1 = 1$.\\
            \textbf{Case 1:} $\mu_n = 1/|a_n| + 1/|b_n|$\\
                Here we want to have $v_n$ orthogonal to $v_{n+1}$.
                Set
                \begin{align*}
                    V_n &:= \max\left(M_n, \frac{1}{\sqrt{\smash[b]{|a_n|}}}\right),\\
                    U_n &:= \left(\frac{1}{a_n^2 V_n^2} + \frac{1}{b_n^2 V_{n+1}^2}\right)^\frac{1}{2},\\
                    M_{n+1} &:= \frac{1}{\sqrt{\smash[b]{|b_n|}}}.
                \end{align*}
                \begin{prop}
                    The following inequalities are true:
                    \begin{align*}
                        M_{n+1} &\leq \sqrt{\mu_n},\\
                        U_n &\leq \sqrt{\mu_n},\\
                        V_n &\leq \max(\sqrt{\mu_n}, M_n).
                    \end{align*}
                    There exist such angles $\beta_n$, $\gamma_n$ that with the values $U_n$, $V_n$ defined like this the following is true:
                    \begin{align*}
                        \inner{u_n}{v_n} &= 1/a_n,\\
                        \inner{u_n}{v_{n+1}} &= 1/b_n,\\
                        \inner{v_n}{v_{n+1}} &= 0.
                    \end{align*}
                \end{prop}
                \begin{proof}
                    First part of the proposition is a simple exercise.\\
                    Now we want to understand why there exist such $\beta_n$ and $\gamma_n$ that all the scalar products of
                    $v_n$, $v_{n+1}$, $u_n$ satisfy the conditions above.
                    Now observe that the chosen $U_n$, $1/(|a_n| V_n)$ and $1/(|b_n| V_{n+1})$ 
                    comprise a right triangle with $U_n$ as a hypotenuse.
                    As a result there always be such $\beta_n$ and $\gamma_n$ that
                    \begin{align*}
                        |U_n \cos{\beta_n}| &= \frac{1}{|a_n|V_n},\\
                        \left|U_n \cos{\gamma_n}\right| &= \left|U_n \sin{\beta_n}\right| = \frac{1}{|b_n|V_{n+1}}.
                    \end{align*}
                \end{proof}
            \noindent\textbf{Case 2:} $\mu_n = (1 + |a_n|)/|c_n|$\\
                Here we will have $u_n$ orthogonal to $v_n$.
                Assign:
                \begin{align*}
                    M_{n+1} &:= \max\biggl(\smash[b]{\frac{\sqrt{|a_n|}}{\sqrt{|c_n|}}}, \frac{1}{\sqrt{\smash[b]{|c_n|}}}\biggr),\\
                    V_n &:= \max\left(M_n, \frac{2}{\sqrt{\smash[b]{|c_n|}}}\right),\\
                    \alpha_n &:= \arccos{\frac{1}{c_n V_n V_{n+1}}},\\
                    \gamma_n &:= \frac{\pi}{2} \pm \alpha_n,\\
                    U_n &:= \frac{a_n}{c_n \cos{\gamma_n} V_{n+1}}.
                \end{align*}
                \begin{remark*}
                    We choose plus or minus in the expression of $\gamma_n$ in order to make the $a_n/(c_n \cos{\gamma_n})$ always positive.
                \end{remark*}
                \begin{prop}
                    The angle $\alpha_n$ is defined correctly and the following inequalities are true:
                    \begin{align*}
                        M_{n+1} &\leq \sqrt{\mu_n},\\
                        V_n &\leq \max(2\sqrt{\mu_n}, M_n),\\
                        U_n &\leq \sqrt{\mu_n}.
                    \end{align*}
                    With the values $U_n$, $V_n$, $\gamma_n$, $\alpha_n$ defined like this the following is true:
                    \begin{align*}
                        \inner{u_n}{v_n} &= 0,\\
                        \inner{u_n}{v_{n+1}} &= a_n/c_n,\\
                        \inner{v_n}{v_{n+1}} &= 1/c_n.
                    \end{align*}
                \end{prop}
                \begin{proof}
                    Firstly $\alpha_n$ is defined correctly, since
                    $$
                    V_n V_{n+1} \geq V_n M_{n+1} \geq \frac{2}{\sqrt{\smash[b]{|c_n|}}} \frac{1}{\sqrt{\smash[b]{|c_n|}}}
                     = \frac{2}{|c_n|}
                    $$
                    which means that the absolute value of the arccosinus argument is always less than $1/2$. Now taking into
                    account that $|\cos{\alpha_n}|$ is less than or equal to $1/2$, we have that $|\cos{\gamma_n}| = |\sin{\alpha_n}|$ is always greater than $1/2$. It is the fact which will use later on in this proof.
                    
                    Trivially $M_{n+1} \leq \sqrt{\mu_n}$ and $V_n \leq \max(2\sqrt{\mu_n}, M_n)$. Next
                    $$
                        U_n = |U_n| \leq 2 \frac{|a_n|}{|c_n|} \frac{1}{V_{n+1}} \leq 2 \frac{\sqrt{|a_n|}}{\sqrt{|c_n|}} \leq \sqrt{\mu_n}.
                    $$
                    It is easy to notice that the angles and $U_n$ are chosen in such way that the scalar product conditions are 
                    satisfied.
                    Finally, we are able to lay out the vectors $v_n$, $u_n$ and $v_{n+1}$ with the prescribed angles 
                    since we have $\beta_n = \pi/2 = (\pi/2 \pm \alpha_n) \mp \alpha_n = \gamma_n \mp \alpha_n$,
                    where we choose the '$\mp$' sign accordingly to our choice in the expression of the $\gamma_n$ angle.
                \end{proof}
            \noindent\textbf{Case 3:} $\mu_n = (1 + |b_n|)/|d_n|$\\
                This case is almost identical to the previous one.\\
                Here we will have $u_n$ orthogonal to $v_{n+1}$.
                Assign:
                \begin{align*}
                    M_{n+1} &:= \max\biggl(\smash[t]{\frac{\sqrt{|b_n|}}{\sqrt{|d_n|}}}, \frac{1}{\sqrt{\smash[b]{|d_n|}}}\biggr),\\
                    V_n &:= \max\left(M_n, \frac{2}{\sqrt{\smash[b]{|d_n|}}}\right),\\
                    \alpha_n &:= \arccos{\frac{1}{-d_n V_n V_{n+1}}},\\
                    \beta_n &:= \frac{\pi}{2} \pm \alpha_n,\\
                    U_n &:= \frac{b_n}{d_n \cos{\beta_n} V_n}.
                \end{align*}
                \begin{remark*}
                    We choose plus or minus in the expression of $\beta_n$ in order to make the $b_n/(d_n \cos{\beta_n})$ always positive.
                \end{remark*}
                \begin{prop}
                    The angle $\alpha_n$ is defined correctly and the following inequalities are true:
                    \begin{align*}
                        M_{n+1} &\leq \sqrt{\mu_n},\\
                        V_n &\leq \max(2\sqrt{\mu_n}, M_n),\\
                        U_n &\leq \sqrt{\mu_n}.
                    \end{align*}
                    With the values $U_n$, $V_n$, $\beta_n$, $\alpha_n$ defined like this the following is true:
                    \begin{align*}
                        \inner{u_n}{v_n} &= b_n/d_n,\\
                        \inner{u_n}{v_{n+1}} &= 0,\\
                        \inner{v_n}{v_{n+1}} &= -1/d_n.
                    \end{align*}
                \end{prop}
                \begin{proof}
                    Firstly $\alpha_n$ is defined correctly, since
                    $$
                    V_n V_{n+1} \geq V_n M_{n+1} \geq \frac{2}{\sqrt{\smash[b]{|d_n|}}} \frac{1}{\sqrt{\smash[b]{|d_n|}}}
                     = \frac{2}{|d_n|}
                    $$
                    which means that the absolute value of the arccosinus argument is always less than $1/2$. Now taking into
                    account that $|\cos{\alpha_n}|$ is less than or equal to $1/2$, we always have $|\cos{\beta_n}| > 1/2$.
                    
                    Trivially $M_{n+1} \leq \sqrt{\mu_n}$ and $V_n \leq \max(2\sqrt{\mu_n}, M_n)$. Next
                    $$
                        U_n = |U_n| \leq 2 \frac{|b_n|}{|d_n|} \frac{1}{V_n} \leq 2 \frac{\sqrt{|b_n|}}{\sqrt{|d_n|}} \leq \sqrt{\mu_n}.
                    $$
                    It is easy to notice that the angles and $U_n$ are chosen in such way that the scalar product conditions are 
                    satisfied.
                    Finally, we are able to lay out the vectors $v_n$, $u_n$ and $v_{n+1}$ with the prescribed angles 
                    since we have $\gamma_n = \pi/2 = (\pi/2 \pm \alpha_n) \mp \alpha_n = \beta_n \mp \alpha_n$,
                    where we choose the '$\mp$' sign accordingly to our choice in the expression of the $\beta_n$ angle.
                \end{proof}
            In each of three cases we guaranteed that $M_{n+1} \leq \sqrt{\mu_n}$ and
            that $V_n \leq \max(M_n, 2\sqrt{\mu_n})$. Hence we get that $V_n$ is bounded up to some constant by the $\max(\sqrt{\mu_{n-1}}, \sqrt{\mu_n})$. Due to all three propositions we also have an estimated bound for $U_n$: we always get
            $U_n \leq \sqrt{\mu_n}$. Thus the constructed vector lenghs sequences $V_n$ and $U_n$ belong to $l^2$.\\
            In the end let us choose such $v_1^*$ that $\inner{v_1}{v_1^*}$ is equal to $-1$.
            It is possible since we earlier set $M_1 = 1$ which means that the vector $v_1$ is not trivial, because it has
            a length greater than zero.
            Additionally we set all the other $v^*_n$ and $u^*_n$ to zero in order to guarantee that for any $n > 1$ we have $\Xi_n$ equal to $1$. The trace of such operator is obviously equal to $-1$. The proposition is proven.
        \end{proof}
        The proof of the theorem is finished with the proof of the statements~\ref{inf-dim-statement} and~\ref{k-dim-statement}.
    \end{proof}
    
    
\medskip
% E-mail: aapyshkin@gmail.com
\bigskip
\begin {thebibliography}{20}
    \bibitem{azoff}
    E.~\!Azoff, H.~\!Shehada,
    \emph{Algebras generated by mutually orthogonal idempotent operators}.
    J. Oper. Theory, 29 (1993), 2, 249--267.
    \bibitem{bbb} 
    A. Baranov, Yu. Belov, A. Borichev,                                       
    \emph{Hereditary completeness for systems of exponentials and reproducing kernels},
    Adv. Math., 235 (2013), 1, 525--554.
    \bibitem{bbb1}
    A. Baranov, Yu. Belov, A. Borichev, 
    \emph{Spectral synthesis in de Branges spaces},
    Geom. Funct. Anal. (GAFA), 25 (2015), 2, 417--452.
    \bibitem{ad_preprint}
    A.D.~\!Baranov, D.V.~\!Yakubovich,
    \emph{Completeness and spectral synthesis of nonselfadjoint one-dimensional
    perturbations of selfadjoint operators}.
    arXiv:1212.5965 [math.FA]
    \bibitem{katavolos}
    A.~\!Katavolos, M.~\!Lambrou, M.~\!Papadakis,
    \emph{On some algebras diagonalized by $M$-bases of $\ell^2$}.
    Integr. Equat. Oper. Theory, 17 (1993), 1, 68--94.
    %\bibitem{wermer}
    %J.~\!Wermer,
    %\emph{On invariant subspaces of normal operators}.
    %Proc. Amer. Math. Soc., 3(1952), 2, 270--277.
    \bibitem{larson}
    D.~\!Larson, W.~\!Wogen,
    \emph{Reflexivity properties of $T\bigoplus0$}.
    J. Funct. Anal., 92 (1990), 448--467.
    %\bibitem{rotfeld}
    %В.В.~\!Пеллер,
    %\emph{Операторы Ганкеля и их приложения}.
    %Издательство РХД, Ижевск(2005).
    %N.K.~\!Nikol'skii,
    %\emph{Complete extensions of Volterra operators},
    %Izv. Akad. Nauk SSSR Ser. Mat 33(1969), 1349--1355. (Russian)

\end{thebibliography}
\vspace{1em}
\noindent{\bf Keywords:} minimal system, biorthogonal system, hereditary completeness, strong M-basis, summation method.

\end{document}
